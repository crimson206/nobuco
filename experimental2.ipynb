{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_between(text, start, end, multiple=False):\n",
    "    target = r'(.*?)'\n",
    "    pattern = re.escape(start) + target + re.escape(end)\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    if multiple:\n",
    "        results = []\n",
    "        for match in matches:\n",
    "            results.append([item.strip() for item in match.strip().split(',')])\n",
    "        return results\n",
    "    else:\n",
    "        if matches:\n",
    "            return [item.strip() for item in matches[0].strip().split(',')]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def get_targets_from_path(path):\n",
    "    with open(path, 'r') as file:\n",
    "            content = file.read()\n",
    "    start = '@converter('\n",
    "    end = ', channel_ordering_strategy'\n",
    "    extract_between(content, start, end, multiple=True)\n",
    "\n",
    "path = \"/home/crimson/Projects/AI-cookbook/nobuco/nobuco/node_converters/math.py\"\n",
    "# targets = get_targets_from_path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'r') as file:\n",
    "        content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_content(path):\n",
    "    with open(path, 'r') as file:\n",
    "            content = file.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"extract_doc\")\n",
    "logging.basicConfig(filename=f\"{logger.name}.log\",\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def extract_doc(target):\n",
    "    split = str(target).split(\".\")\n",
    "    if \"Tensor\" in split:\n",
    "        docs = getattr(torch.Tensor, split[-1]).__doc__\n",
    "    elif \"F\" in split:\n",
    "        try:\n",
    "            docs = getattr(torch.nn.functional, split[-1]).__doc__\n",
    "        except:\n",
    "            try:\n",
    "                docs = getattr(torch.functional, split[-1]).__doc__\n",
    "            except:\n",
    "                docs = None\n",
    "    elif \"nn\" in split:\n",
    "        docs = getattr(torch.nn, split[-1]).__doc__\n",
    "    else:\n",
    "        docs = getattr(torch, split[-1]).__doc__\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Any, Dict\n",
    "\n",
    "class Session(BaseModel):\n",
    "    header: str\n",
    "    content: str\n",
    "    \n",
    "class Item(BaseModel):\n",
    "    name: str\n",
    "    docs: str\n",
    "    parsed_docs: List[Session] = []\n",
    "\n",
    "class ItemHolder(BaseModel):\n",
    "    file_name: str\n",
    "    items: List[Item] = []\n",
    "    failed_items: List[str] = []\n",
    "    meta_data: Dict[str, Any] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_docs(parsed_docs):\n",
    "    filtered = []\n",
    "    for session in parsed_docs:\n",
    "        if session[\"header\"] in [\"Args\", \"Shape\", \"Examples\", \"Example\"]:\n",
    "            filtered.append(session)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_parsed_to_str(parsed_docs):\n",
    "    formatted_string = \"\"\n",
    "\n",
    "    for item in parsed_docs:\n",
    "        formatted_string += item['header'] + '\\n'\n",
    "        formatted_string += '    ' + item['content'].replace('\\n', '\\n    ') + '\\n\\n'\n",
    "    return formatted_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sections(text, header_indent, unit_indent):\n",
    "    lines = text.split('\\n')\n",
    "    parsed_data = []\n",
    "    current_header = None\n",
    "    current_content = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Remove leading indentation\n",
    "        line = line[header_indent:]\n",
    "\n",
    "        # Check if the line is a header\n",
    "        if line and line[0].isalpha() and line.strip().endswith(':'):\n",
    "            # If there's a current header, save its content before starting a new one\n",
    "            if current_header is not None:\n",
    "                parsed_data.append({'header': current_header, 'content': '\\n'.join(current_content)})\n",
    "                current_content = []\n",
    "            # Set new header\n",
    "            current_header = line.rstrip(':')\n",
    "        else:\n",
    "            if current_header is not None:\n",
    "                # Add line to current content\n",
    "                current_content.append(line[unit_indent:])\n",
    "\n",
    "    # Don't forget to save the last header-content pair\n",
    "    if current_header is not None:\n",
    "        parsed_data.append({'header': current_header, 'content': '\\n'.join(current_content)})\n",
    "\n",
    "    return parsed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_content(path):\n",
    "    with open(path, 'r') as file:\n",
    "            content = file.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemholder = ItemHolder(\n",
    "    file_name=\"math.py\"\n",
    ")\n",
    "\n",
    "\n",
    "content = load_content(\"/home/crimson/Projects/AI-cookbook/nobuco/nobuco/node_converters/math.py\")\n",
    "\n",
    "start = '@converter('\n",
    "end = ', channel_ordering_strategy'\n",
    "targets = extract_between(content, start, end, multiple=True)\n",
    "\n",
    "for target in targets:\n",
    "    try:\n",
    "        docs = extract_doc(target[0])\n",
    "        item = Item(\n",
    "            name=str(target[0]),\n",
    "            docs=docs,\n",
    "        )\n",
    "        itemholder.items.append(item)\n",
    "    except Exception as error:\n",
    "        itemholder.failed_items.append(str(target[0]))\n",
    "        logger.error(error)\n",
    "\n",
    "for item in itemholder.items:\n",
    "    func_type = parse_sections(item.docs, 0, 4)\n",
    "    cls_type = parse_sections(item.docs, 4, 4)\n",
    "    func_type_headers = [session[\"header\"] for session in func_type]\n",
    "    if any(keyword in func_type_headers for keyword in [\"Example\", \"Examples\", \"Args\"]):\n",
    "        item.parsed_docs = func_type\n",
    "    else:\n",
    "        item.parsed_docs = cls_type\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_target_directory(root_dir, target_hint):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if target_hint in dirpath:\n",
    "            return dirpath\n",
    "    return None\n",
    "\n",
    "def get_preprocessed_itemholder(file_name, target_hint=\"nobuco/node_converters\"):\n",
    "    # Get the current working directory\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # Find the target directory that contains the hint\n",
    "    target_directory = find_target_directory(cwd, target_hint)\n",
    "    \n",
    "    if target_directory:\n",
    "        # Construct the full path\n",
    "        full_path = os.path.join(target_directory, file_name)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Directory containing hint '{target_hint}' not found from {cwd}\")\n",
    "    \n",
    "    content = load_content(full_path)\n",
    "    \n",
    "    start = '@converter('\n",
    "    end = ', channel_ordering_strategy'\n",
    "    targets = extract_between(content, start, end, multiple=True)\n",
    "\n",
    "    itemholder = ItemHolder(\n",
    "        file_name=file_name\n",
    "    )\n",
    "\n",
    "    for target in targets:\n",
    "        try:\n",
    "            docs = extract_doc(target[0])\n",
    "            item = Item(\n",
    "                name=str(target[0]),\n",
    "                docs=docs,\n",
    "            )\n",
    "            itemholder.items.append(item)\n",
    "        except Exception as error:\n",
    "            itemholder.failed_items.append(str(target[0]))\n",
    "            logger.error(error)\n",
    "\n",
    "    for item in itemholder.items:\n",
    "        func_type = parse_sections(item.docs, 0, 4)\n",
    "        cls_type = parse_sections(item.docs, 4, 4)\n",
    "        func_type_headers = [session[\"header\"] for session in func_type]\n",
    "        if any(keyword in func_type_headers for keyword in [\"Example\", \"Examples\", \"Args\"]):\n",
    "            item.parsed_docs = func_type\n",
    "        else:\n",
    "            item.parsed_docs = cls_type\n",
    "\n",
    "    return itemholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemholder = get_preprocessed_itemholder(\"math.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['torch.Tensor.__rsub__',\n",
       " 'torch.Tensor.__truediv__',\n",
       " 'torch.Tensor.__rdiv__',\n",
       " 'torch.Tensor.__mod__',\n",
       " 'torch.Tensor.__rpow__',\n",
       " 'torch.clamp_min',\n",
       " 'torch.clamp_max']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemholder.failed_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'x1 (Tensor): input tensor of shape :math:`B \\\\times P \\\\times '\n",
      "             'M`.\\n'\n",
      "             'x2 (Tensor): input tensor of shape :math:`B \\\\times R \\\\times '\n",
      "             'M`.\\n'\n",
      "             'p: p value for the p-norm distance to calculate between each '\n",
      "             'vector pair\\n'\n",
      "             '    :math:`\\\\in [0, \\\\infty]`.\\n'\n",
      "             'compute_mode:\\n'\n",
      "             \"    'use_mm_for_euclid_dist_if_necessary' - will use matrix \"\n",
      "             'multiplication approach to calculate\\n'\n",
      "             '    euclidean distance (p = 2) if P > 25 or R > 25\\n'\n",
      "             \"    'use_mm_for_euclid_dist' - will always use matrix \"\n",
      "             'multiplication approach to calculate\\n'\n",
      "             '    euclidean distance (p = 2)\\n'\n",
      "             \"    'donot_use_mm_for_euclid_dist' - will never use matrix \"\n",
      "             'multiplication approach to calculate\\n'\n",
      "             '    euclidean distance (p = 2)\\n'\n",
      "             '    Default: use_mm_for_euclid_dist_if_necessary.\\n'\n",
      "             '\\n'\n",
      "             '1 has shape :math:`B \\\\times P \\\\times M` and x2 has shape '\n",
      "             ':math:`B \\\\times R \\\\times M` then the\\n'\n",
      "             'ut will have shape :math:`B \\\\times P \\\\times R`.\\n'\n",
      "             '\\n'\n",
      "             ' function is equivalent to '\n",
      "             \"`scipy.spatial.distance.cdist(input,'minkowski', p=p)`\\n\"\n",
      "             'math:`p \\\\in (0, \\\\infty)`. When :math:`p = 0` it is equivalent '\n",
      "             'to\\n'\n",
      "             \"py.spatial.distance.cdist(input, 'hamming') * M`. When :math:`p \"\n",
      "             '= \\\\infty`, the closest\\n'\n",
      "             'y function is `scipy.spatial.distance.cdist(xn, lambda x, y: '\n",
      "             'np.abs(x - y).max())`.\\n',\n",
      "  'header': 'Args'},\n",
      " {'content': '\\n'\n",
      "             '>>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], '\n",
      "             '[-0.4821,  1.059]])\\n'\n",
      "             '>>> a\\n'\n",
      "             'tensor([[ 0.9041,  0.0196],\\n'\n",
      "             '        [-0.3108, -2.4423],\\n'\n",
      "             '        [-0.4821,  1.0590]])\\n'\n",
      "             '>>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\\n'\n",
      "             '>>> b\\n'\n",
      "             'tensor([[-2.1763, -0.4713],\\n'\n",
      "             '        [-0.6986,  1.3702]])\\n'\n",
      "             '>>> torch.cdist(a, b, p=2)\\n'\n",
      "             'tensor([[3.1193, 2.0959],\\n'\n",
      "             '        [2.7138, 3.8322],\\n'\n",
      "             '        [2.2830, 0.3791]])\\n',\n",
      "  'header': 'Example'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(item.parsed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from func_template import phase1_example, phase1_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(template, target, doc, example):\n",
    "    processed_docs = convert_parsed_to_str(filter_docs(doc))\n",
    "    \n",
    "    prompt = template.format(\n",
    "        target=target,\n",
    "        doc=processed_docs,\n",
    "        phase1_example=example\n",
    "    )\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = itemholder.items[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\nother (Tensor or Number): the tensor or number to add to :attr:`input`.\\n'}, {'header': 'Keyword arguments', 'content': 'alpha (Number): the multiplier for :attr:`other`.\\nout (Tensor, optional): the output tensor.\\n'}, {'header': 'Examples', 'content': '\\n>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.0202,  1.0985,  1.3506, -0.6056])\\n>>> torch.add(a, 20)\\ntensor([ 20.0202,  21.0985,  21.3506,  19.3944])\\n\\n>>> b = torch.randn(4)\\n>>> b\\ntensor([-0.9732, -0.3497,  0.6245,  0.4022])\\n>>> c = torch.randn(4, 1)\\n>>> c\\ntensor([[ 0.3743],\\n        [-1.7724],\\n        [-0.5811],\\n        [-0.8017]])\\n>>> torch.add(b, c, alpha=10)\\ntensor([[  2.7695,   3.3930,   4.3672,   4.1450],\\n        [-18.6971, -18.0736, -17.0994, -17.3216],\\n        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\\n        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])\\n'}], 'torch.add')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.parsed_docs, item.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<<Instruction>>\n",
      "\n",
      "Given the context, complete the <<Task>>.\n",
      "\n",
      "<<Documentation for torch.add>>\n",
      "\n",
      "Args\n",
      "    input (Tensor): the input tensor.\n",
      "    other (Tensor or Number): the tensor or number to add to :attr:`input`.\n",
      "    \n",
      "\n",
      "Examples\n",
      "    \n",
      "    >>> a = torch.randn(4)\n",
      "    >>> a\n",
      "    tensor([ 0.0202,  1.0985,  1.3506, -0.6056])\n",
      "    >>> torch.add(a, 20)\n",
      "    tensor([ 20.0202,  21.0985,  21.3506,  19.3944])\n",
      "    \n",
      "    >>> b = torch.randn(4)\n",
      "    >>> b\n",
      "    tensor([-0.9732, -0.3497,  0.6245,  0.4022])\n",
      "    >>> c = torch.randn(4, 1)\n",
      "    >>> c\n",
      "    tensor([[ 0.3743],\n",
      "            [-1.7724],\n",
      "            [-0.5811],\n",
      "            [-0.8017]])\n",
      "    >>> torch.add(b, c, alpha=10)\n",
      "    tensor([[  2.7695,   3.3930,   4.3672,   4.1450],\n",
      "            [-18.6971, -18.0736, -17.0994, -17.3216],\n",
      "            [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\n",
      "            [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "<<Example>>\n",
      "\n",
      "Generate simple inputs to run the target code block.\n",
      "\n",
      "target:\n",
      "```\n",
      "torch.Tensor.add(*inputs)\n",
      "```\n",
      "\n",
      "output:\n",
      "```python\n",
      "inputs = [\n",
      "    torch.randn(1, 20, 32),\n",
      "    torch.randn(1, 20, 32)\n",
      "]\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "<<Task>>\n",
      "Generate simple inputs to run the target code block.\n",
      "\n",
      "target:\n",
      "```python\n",
      "torch.add\n",
      "```\n",
      "\n",
      "output:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_prompt(phase1_prompt, item.name, item.parsed_docs, phase1_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computes batched the p-norm distance between each pair of the two collections of row vectors.\n",
      "\n",
      "    Args:\n",
      "        x1 (Tensor): input tensor of shape :math:`B \\times P \\times M`.\n",
      "        x2 (Tensor): input tensor of shape :math:`B \\times R \\times M`.\n",
      "        p: p value for the p-norm distance to calculate between each vector pair\n",
      "            :math:`\\in [0, \\infty]`.\n",
      "        compute_mode:\n",
      "            'use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate\n",
      "            euclidean distance (p = 2) if P > 25 or R > 25\n",
      "            'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate\n",
      "            euclidean distance (p = 2)\n",
      "            'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate\n",
      "            euclidean distance (p = 2)\n",
      "            Default: use_mm_for_euclid_dist_if_necessary.\n",
      "\n",
      "    If x1 has shape :math:`B \\times P \\times M` and x2 has shape :math:`B \\times R \\times M` then the\n",
      "    output will have shape :math:`B \\times P \\times R`.\n",
      "\n",
      "    This function is equivalent to `scipy.spatial.distance.cdist(input,'minkowski', p=p)`\n",
      "    if :math:`p \\in (0, \\infty)`. When :math:`p = 0` it is equivalent to\n",
      "    `scipy.spatial.distance.cdist(input, 'hamming') * M`. When :math:`p = \\infty`, the closest\n",
      "    scipy function is `scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max())`.\n",
      "\n",
      "    Example:\n",
      "\n",
      "        >>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\n",
      "        >>> a\n",
      "        tensor([[ 0.9041,  0.0196],\n",
      "                [-0.3108, -2.4423],\n",
      "                [-0.4821,  1.0590]])\n",
      "        >>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\n",
      "        >>> b\n",
      "        tensor([[-2.1763, -0.4713],\n",
      "                [-0.6986,  1.3702]])\n",
      "        >>> torch.cdist(a, b, p=2)\n",
      "        tensor([[3.1193, 2.0959],\n",
      "                [2.7138, 3.8322],\n",
      "                [2.2830, 0.3791]])\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(torch.cdist.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in itemholder.items:\n",
    "    processed_docs = convert_parsed_to_str(filter_docs(item.parsed_docs))\n",
    "    \n",
    "    phase1_prompt.format(\n",
    "        target=item.name,\n",
    "        doc=processed_docs,\n",
    "        phase1_example=phase1_example\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itemholder.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['torch.Tensor.__rsub__',\n",
       " 'torch.Tensor.__truediv__',\n",
       " 'torch.Tensor.__rdiv__',\n",
       " 'torch.Tensor.__mod__',\n",
       " 'torch.Tensor.__rpow__',\n",
       " 'torch.clamp_min',\n",
       " 'torch.clamp_max']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemholder.failed_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in itemholder.items:\n",
    "    item.meta_data[\"prompt\"] = generate_prompt(phase1_prompt, item.name, item.parsed_docs, phase1_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "    model = \"llama3:70b-instruct-q2_K\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=model, \n",
    "        messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "    },\n",
    "    ])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"mongodb+srv://sisungkim:skrkwh8327@cluster0.mkj2sh1.mongodb.net/\"\n",
    "import pymongo\n",
    "\n",
    "myclient = pymongo.MongoClient(connection_string)\n",
    "\n",
    "def insert_data(db_name, col_name, data):\n",
    "    mydb = myclient[db_name]\n",
    "    mycol = mydb[col_name]\n",
    "    response = mycol.insert_one(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_db(item, phase1_example, generated_text):\n",
    "\n",
    "    inputgeneration = {\n",
    "        \"project_name\": \"unittest_automation_test_initiation\",\n",
    "        \"project_id\": \"0\",\n",
    "        \"task\": \"function_input_generation\",\n",
    "        \"name\":item.name,\n",
    "        \"example\":phase1_example,\n",
    "        \"docs\": item.parsed_docs,\n",
    "        \"generated_text\": generated_text,\n",
    "    }\n",
    "\n",
    "    mycol.insert_one(inputgeneration)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
