{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Function Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "def extract_between(text, start, end, extend=False, multiple=False):\n",
    "    target = r'(.*)' if extend else r'(.*?)'\n",
    "    pattern = re.escape(start) + target + re.escape(end)\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    if multiple:\n",
    "        results = []\n",
    "        for match in matches:\n",
    "            results.append(match.strip())\n",
    "        return results\n",
    "    else:\n",
    "        # Return the first match if available, else None\n",
    "        return matches[0].strip() if matches else None\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "@converter(torch.sin, torch.Tensor.sin, channel_ordering_strategy=ChannelOrderingStrategy.MINIMUM_TRANSPOSITIONS)\n",
    "def converter_sin(input, *args, **kwargs):\n",
    "    def func(input, *args, **kwargs):\n",
    "        return tf.math.sin(input)\n",
    "    return func\n",
    "\n",
    "\n",
    "@converter(torch.cos, torch.Tensor.cos, channel_ordering_strategy=ChannelOrderingStrategy.MINIMUM_TRANSPOSITIONS)\n",
    "def converter_cos(input, *args, **kwargs):\n",
    "    def func(input, *args, **kwargs):\n",
    "        return tf.math.cos(input)\n",
    "    return func\n",
    "\n",
    "\n",
    "@converter(torch.add, torch.Tensor.add, torch.Tensor.add_, torch.Tensor.__add__, torch.Tensor.__iadd__, torch.Tensor.__radd__, channel_ordering_strategy=ChannelOrderingStrategy.MINIMUM_TRANSPOSITIONS_OR_PYTORCH, autocast=True)\n",
    "def converter_add(input, other, *args, **kwargs):\n",
    "    def func(input, other, *args, **kwargs):\n",
    "        return input + other\n",
    "    return func\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end = ', channel_ordering_strategy'\n",
    "\n",
    "It makes the `extend` argument useless, but free from the additional arguments to the target functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extend: True multiple:  True\n",
      "['torch.sin, torch.Tensor.sin',\n",
      " 'torch.cos, torch.Tensor.cos',\n",
      " 'torch.add, torch.Tensor.add, torch.Tensor.add_, torch.Tensor.__add__, '\n",
      " 'torch.Tensor.__iadd__, torch.Tensor.__radd__']\n",
      "\n",
      "\n",
      "extend: True multiple:  False\n",
      "'torch.sin, torch.Tensor.sin'\n",
      "\n",
      "\n",
      "extend: False multiple:  True\n",
      "['torch.sin, torch.Tensor.sin',\n",
      " 'torch.cos, torch.Tensor.cos',\n",
      " 'torch.add, torch.Tensor.add, torch.Tensor.add_, torch.Tensor.__add__, '\n",
      " 'torch.Tensor.__iadd__, torch.Tensor.__radd__']\n",
      "\n",
      "\n",
      "extend: False multiple:  False\n",
      "'torch.sin, torch.Tensor.sin'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "start = '@converter('\n",
    "end = ', channel_ordering_strategy'\n",
    "\n",
    "bools = [True, False]\n",
    "\n",
    "for _bool_ex in bools:\n",
    "\textend = _bool_ex\n",
    "\tfor _bool_mul in bools:\n",
    "\t\tmultiple = _bool_mul  \n",
    "\t\tprint(\"extend:\", str(_bool_ex), \"multiple: \", str(_bool_mul))\n",
    "\t\tpprint(extract_between(text, start, end, extend=extend, multiple=multiple))\n",
    "\t\tprint(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end = ','\n",
    "\n",
    "It makes the `extend` argument functional, but it can include non-target results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extend: True multiple:  True\n",
      "['torch.sin, torch.Tensor.sin',\n",
      " 'torch.cos, torch.Tensor.cos',\n",
      " 'torch.add, torch.Tensor.add, torch.Tensor.add_, torch.Tensor.__add__, '\n",
      " 'torch.Tensor.__iadd__, torch.Tensor.__radd__, '\n",
      " 'channel_ordering_strategy=ChannelOrderingStrategy.MINIMUM_TRANSPOSITIONS_OR_PYTORCH']\n",
      "\n",
      "\n",
      "extend: True multiple:  False\n",
      "'torch.sin, torch.Tensor.sin'\n",
      "\n",
      "\n",
      "extend: False multiple:  True\n",
      "['torch.sin', 'torch.cos', 'torch.add']\n",
      "\n",
      "\n",
      "extend: False multiple:  False\n",
      "'torch.sin'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "start = '@converter('\n",
    "end = ','\n",
    "\n",
    "bools = [True, False]\n",
    "\n",
    "for _bool_ex in bools:\n",
    "\textend = _bool_ex\n",
    "\tfor _bool_mul in bools:\n",
    "\t\tmultiple = _bool_mul  \n",
    "\t\tprint(\"extend:\", str(_bool_ex), \"multiple: \", str(_bool_mul))\n",
    "\t\tpprint(extract_between(text, start, end, extend=extend, multiple=multiple))\n",
    "\t\tprint(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation2\n",
    "\n",
    "- discard `extend` argument\n",
    "- use out[0] for single output instead\n",
    "- split each result as `list(str)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_between(text, start, end, multiple=False):\n",
    "    target = r'(.*?)'\n",
    "    pattern = re.escape(start) + target + re.escape(end)\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    if multiple:\n",
    "        results = []\n",
    "        for match in matches:\n",
    "            results.append([item.strip() for item in match.strip().split(',')])\n",
    "        return results\n",
    "    else:\n",
    "        if matches:\n",
    "            return [item.strip() for item in matches[0].strip().split(',')]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple:  True\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pprint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m multiple \u001b[38;5;241m=\u001b[39m _bool_mul  \n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(_bool_mul))\n\u001b[0;32m---> 10\u001b[0m \u001b[43mpprint\u001b[49m(extract_between(text, start, end, multiple\u001b[38;5;241m=\u001b[39mmultiple))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pprint' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "start = '@converter('\n",
    "end = ', channel_ordering_strategy'\n",
    "\n",
    "bools = [True, False]\n",
    "\n",
    "for _bool_mul in bools:\n",
    "\tmultiple = _bool_mul  \n",
    "\tprint(\"multiple: \", str(_bool_mul))\n",
    "\tpprint(extract_between(text, start, end, multiple=multiple))\n",
    "\tprint(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Select the Implementation2 because it is more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `__doc__` Extration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "text = \"\"\"\n",
    "from typing import Optional, Union, List, Tuple, Sequence, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.types import _int, _bool, Number, _dtype, _size, _layout, _device\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.layers\n",
    "\n",
    "from nobuco.commons import ChannelOrder, ChannelOrderingStrategy\n",
    "from nobuco.converters.channel_ordering import set_channel_order, get_channel_order\n",
    "from nobuco.converters.node_converter import converter\n",
    "from nobuco.converters.tensor import _dim_make_positive, dim_pytorch2keras, perm_keras2pytorch, _permute, \\\n",
    "    perm_pytorch2keras, _ensure_iterable, _dims_make_positive, dims_pytorch2keras\n",
    "from nobuco.converters.type_cast import dtype_pytorch2keras\n",
    "\n",
    "\n",
    "@converter(torch.sin, torch.Tensor.sin, channel_ordering_strategy=ChannelOrderingStrategy.MINIMUM_TRANSPOSITIONS)\n",
    "def converter_sin(input, *args, **kwargs):\n",
    "    def func(input, *args, **kwargs):\n",
    "        return tf.math.sin(input)\n",
    "    return func\n",
    "\n",
    "\n",
    "@converter(torch.cos, torch.Tensor.cos, channel_ordering_strategy=ChannelOrderingStrategy.MINIMUM_TRANSPOSITIONS)\n",
    "def converter_cos(input, *args, **kwargs):\n",
    "    def func(input, *args, **kwargs):\n",
    "        return tf.math.cos(input)\n",
    "    return func\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Target Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['torch.sin', 'torch.Tensor.sin'], ['torch.cos', 'torch.Tensor.cos']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = extract_between(text, start, end, multiple=True)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__doc__` Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both, function and class have\n",
    "    Args and Example\n",
    "\n",
    "- class have one additional useful information\n",
    "    Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sin(input, *, out=None) -> Tensor\n",
      "\n",
      "Returns a new tensor with the sine of the elements of :attr:`input`.\n",
      "\n",
      ".. math::\n",
      "    \\text{out}_{i} = \\sin(\\text{input}_{i})\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor.\n",
      "\n",
      "Keyword args:\n",
      "    out (Tensor, optional): the output tensor.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> a = torch.randn(4)\n",
      "    >>> a\n",
      "    tensor([-0.5461,  0.1347, -2.7266, -0.2746])\n",
      "    >>> torch.sin(a)\n",
      "    tensor([-0.5194,  0.1343, -0.4032, -0.2711])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_func = getattr(torch, 'sin')\n",
    "print(target_func.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applies a 2D convolution over an input signal composed of several input\n",
      "    planes.\n",
      "\n",
      "    In the simplest case, the output value of the layer with input size\n",
      "    :math:`(N, C_{\\text{in}}, H, W)` and output :math:`(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})`\n",
      "    can be precisely described as:\n",
      "\n",
      "    .. math::\n",
      "        \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n",
      "        \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\n",
      "\n",
      "\n",
      "    where :math:`\\star` is the valid 2D `cross-correlation`_ operator,\n",
      "    :math:`N` is a batch size, :math:`C` denotes a number of channels,\n",
      "    :math:`H` is a height of input planes in pixels, and :math:`W` is\n",
      "    width in pixels.\n",
      "    \n",
      "\n",
      "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "    * :attr:`stride` controls the stride for the cross-correlation, a single\n",
      "      number or a tuple.\n",
      "\n",
      "    * :attr:`padding` controls the amount of padding applied to the input. It\n",
      "      can be either a string {'valid', 'same'} or an int / a tuple of ints giving the\n",
      "      amount of implicit padding applied on both sides.\n",
      "\n",
      "    * :attr:`dilation` controls the spacing between the kernel points; also\n",
      "      known as the à trous algorithm. It is harder to describe, but this `link`_\n",
      "      has a nice visualization of what :attr:`dilation` does.\n",
      "\n",
      "    * :attr:`groups` controls the connections between inputs and outputs.\n",
      "      :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n",
      "      :attr:`groups`. For example,\n",
      "\n",
      "        * At groups=1, all inputs are convolved to all outputs.\n",
      "        * At groups=2, the operation becomes equivalent to having two conv\n",
      "          layers side by side, each seeing half the input channels\n",
      "          and producing half the output channels, and both subsequently\n",
      "          concatenated.\n",
      "        * At groups= :attr:`in_channels`, each input channel is convolved with\n",
      "          its own set of filters (of size\n",
      "          :math:`\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}`).\n",
      "\n",
      "    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
      "\n",
      "        - a single ``int`` -- in which case the same value is used for the height and width dimension\n",
      "        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n",
      "          and the second `int` for the width dimension\n",
      "\n",
      "    Note:\n",
      "        When `groups == in_channels` and `out_channels == K * in_channels`,\n",
      "        where `K` is a positive integer, this operation is also known as a \"depthwise convolution\".\n",
      "\n",
      "        In other words, for an input of size :math:`(N, C_{in}, L_{in})`,\n",
      "        a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments\n",
      "        :math:`(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in})`.\n",
      "\n",
      "    Note:\n",
      "        In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
      "\n",
      "    Note:\n",
      "        ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
      "        the input so the output has the shape as the input. However, this mode\n",
      "        doesn't support any stride values other than 1.\n",
      "\n",
      "    Note:\n",
      "        This module supports complex data types i.e. ``complex32, complex64, complex128``.\n",
      "\n",
      "    Args:\n",
      "        in_channels (int): Number of channels in the input image\n",
      "        out_channels (int): Number of channels produced by the convolution\n",
      "        kernel_size (int or tuple): Size of the convolving kernel\n",
      "        stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
      "        padding (int, tuple or str, optional): Padding added to all four sides of\n",
      "            the input. Default: 0\n",
      "        padding_mode (str, optional): ``'zeros'``, ``'reflect'``,\n",
      "            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
      "        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
      "        groups (int, optional): Number of blocked connections from input\n",
      "            channels to output channels. Default: 1\n",
      "        bias (bool, optional): If ``True``, adds a learnable bias to the\n",
      "            output. Default: ``True``\n",
      "    \n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`\n",
      "        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where\n",
      "\n",
      "          .. math::\n",
      "              H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
      "                        \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
      "\n",
      "          .. math::\n",
      "              W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
      "                        \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
      "\n",
      "    Attributes:\n",
      "        weight (Tensor): the learnable weights of the module of shape\n",
      "            :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\n",
      "            :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\n",
      "            The values of these weights are sampled from\n",
      "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "            :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
      "        bias (Tensor):   the learnable bias of the module of shape\n",
      "            (out_channels). If :attr:`bias` is ``True``,\n",
      "            then the values of these weights are\n",
      "            sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "            :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
      "\n",
      "    Examples:\n",
      "\n",
      "        >>> # With square kernels and equal stride\n",
      "        >>> m = nn.Conv2d(16, 33, 3, stride=2)\n",
      "        >>> # non-square kernels and unequal stride and with padding\n",
      "        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
      "        >>> # non-square kernels and unequal stride and with padding and dilation\n",
      "        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
      "        >>> input = torch.randn(20, 16, 50, 100)\n",
      "        >>> output = m(input)\n",
      "\n",
      "    .. _cross-correlation:\n",
      "        https://en.wikipedia.org/wiki/Cross-correlation\n",
      "\n",
      "    .. _link:\n",
      "        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "target_func = getattr(nn, 'Conv2d')\n",
    "print(target_func.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation1\n",
    "\n",
    "- Output is the dictionary type.\n",
    "- Name of headers must be specified.\n",
    "- Find given '{header_name}:' patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = target_func.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_sections(text):\n",
    "    # Define the section headers\n",
    "    headers = ['Args', 'Shape', 'Attributes', 'Example', 'Examples', 'Note']\n",
    "\n",
    "    # Create a regex pattern to match the headers\n",
    "    header_pattern = re.compile(r'^\\s*(' + '|'.join(headers) + r')\\s*:\\s*$', re.MULTILINE)\n",
    "\n",
    "    # Find all matches for headers and their positions\n",
    "    matches = list(header_pattern.finditer(text))\n",
    "\n",
    "    # Initialize the result dictionary\n",
    "    sections = {}\n",
    "\n",
    "    # Iterate over the matches to extract sections\n",
    "    for i in range(len(matches)):\n",
    "        start_header = matches[i]\n",
    "        end_header = matches[i+1] if i+1 < len(matches) else None\n",
    "\n",
    "        # Extract the section name\n",
    "        section_name = start_header.group(1)\n",
    "\n",
    "        # Extract the section content\n",
    "        start_pos = start_header.end()\n",
    "        end_pos = end_header.start() if end_header else len(text)\n",
    "        section_content = text[start_pos:end_pos].strip()\n",
    "\n",
    "        # Add the section to the dictionary\n",
    "        sections[section_name] = section_content\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<re.Match object; span=(2521, 2531), match='\\n    Note:'>,\n",
       " <re.Match object; span=(3000, 3010), match='\\n    Note:'>,\n",
       " <re.Match object; span=(3390, 3400), match='\\n    Note:'>,\n",
       " <re.Match object; span=(3615, 3625), match='\\n    Note:'>,\n",
       " <re.Match object; span=(3717, 3727), match='\\n    Args:'>,\n",
       " <re.Match object; span=(4586, 4602), match='    \\n\\n    Shape:'>,\n",
       " <re.Match object; span=(5237, 5253), match='\\n    Attributes:'>,\n",
       " <re.Match object; span=(6022, 6037), match='\\n    Examples:\\n'>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = ['Args', 'Shape', 'Attributes', 'Example', 'Examples', 'Note']\n",
    "\n",
    "# Create a regex pattern to match the headers\n",
    "header_pattern = re.compile(r'^\\s*(' + '|'.join(headers) + r')\\s*:\\s*$', re.MULTILINE)\n",
    "\n",
    "# Find all matches for headers and their positions\n",
    "matches = list(header_pattern.finditer(target_func.__doc__))\n",
    "\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Args': 'in_channels (int): Number of channels in the input image\\n'\n",
      "         '        out_channels (int): Number of channels produced by the '\n",
      "         'convolution\\n'\n",
      "         '        kernel_size (int or tuple): Size of the convolving kernel\\n'\n",
      "         '        stride (int or tuple, optional): Stride of the convolution. '\n",
      "         'Default: 1\\n'\n",
      "         '        padding (int, tuple or str, optional): Padding added to all '\n",
      "         'four sides of\\n'\n",
      "         '            the input. Default: 0\\n'\n",
      "         \"        padding_mode (str, optional): ``'zeros'``, ``'reflect'``,\\n\"\n",
      "         \"            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\\n\"\n",
      "         '        dilation (int or tuple, optional): Spacing between kernel '\n",
      "         'elements. Default: 1\\n'\n",
      "         '        groups (int, optional): Number of blocked connections from '\n",
      "         'input\\n'\n",
      "         '            channels to output channels. Default: 1\\n'\n",
      "         '        bias (bool, optional): If ``True``, adds a learnable bias to '\n",
      "         'the\\n'\n",
      "         '            output. Default: ``True``',\n",
      " 'Attributes': 'weight (Tensor): the learnable weights of the module of shape\\n'\n",
      "               '            :math:`(\\\\text{out\\\\_channels}, '\n",
      "               '\\\\frac{\\\\text{in\\\\_channels}}{\\\\text{groups}},`\\n'\n",
      "               '            :math:`\\\\text{kernel\\\\_size[0]}, '\n",
      "               '\\\\text{kernel\\\\_size[1]})`.\\n'\n",
      "               '            The values of these weights are sampled from\\n'\n",
      "               '            :math:`\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})` where\\n'\n",
      "               '            :math:`k = \\\\frac{groups}{C_\\\\text{in} * '\n",
      "               '\\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}`\\n'\n",
      "               '        bias (Tensor):   the learnable bias of the module of '\n",
      "               'shape\\n'\n",
      "               '            (out_channels). If :attr:`bias` is ``True``,\\n'\n",
      "               '            then the values of these weights are\\n'\n",
      "               '            sampled from :math:`\\\\mathcal{U}(-\\\\sqrt{k}, '\n",
      "               '\\\\sqrt{k})` where\\n'\n",
      "               '            :math:`k = \\\\frac{groups}{C_\\\\text{in} * '\n",
      "               '\\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}`',\n",
      " 'Examples': '>>> # With square kernels and equal stride\\n'\n",
      "             '        >>> m = nn.Conv2d(16, 33, 3, stride=2)\\n'\n",
      "             '        >>> # non-square kernels and unequal stride and with '\n",
      "             'padding\\n'\n",
      "             '        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), '\n",
      "             'padding=(4, 2))\\n'\n",
      "             '        >>> # non-square kernels and unequal stride and with '\n",
      "             'padding and dilation\\n'\n",
      "             '        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), '\n",
      "             'padding=(4, 2), dilation=(3, 1))\\n'\n",
      "             '        >>> input = torch.randn(20, 16, 50, 100)\\n'\n",
      "             '        >>> output = m(input)\\n'\n",
      "             '\\n'\n",
      "             '    .. _cross-correlation:\\n'\n",
      "             '        https://en.wikipedia.org/wiki/Cross-correlation\\n'\n",
      "             '\\n'\n",
      "             '    .. _link:\\n'\n",
      "             '        '\n",
      "             'https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md',\n",
      " 'Note': 'This module supports complex data types i.e. ``complex32, complex64, '\n",
      "         'complex128``.',\n",
      " 'Shape': '- Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, '\n",
      "          'H_{in}, W_{in})`\\n'\n",
      "          '        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or '\n",
      "          ':math:`(C_{out}, H_{out}, W_{out})`, where\\n'\n",
      "          '\\n'\n",
      "          '          .. math::\\n'\n",
      "          '              H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in}  + 2 \\\\times '\n",
      "          '\\\\text{padding}[0] - \\\\text{dilation}[0]\\n'\n",
      "          '                        \\\\times (\\\\text{kernel\\\\_size}[0] - 1) - '\n",
      "          '1}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloor\\n'\n",
      "          '\\n'\n",
      "          '          .. math::\\n'\n",
      "          '              W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in}  + 2 \\\\times '\n",
      "          '\\\\text{padding}[1] - \\\\text{dilation}[1]\\n'\n",
      "          '                        \\\\times (\\\\text{kernel\\\\_size}[1] - 1) - '\n",
      "          '1}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloor'}\n"
     ]
    }
   ],
   "source": [
    "pprint(parse_sections(target_func.__doc__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation2\n",
    "\n",
    "- Output is jsonl type.\n",
    "- Name of headers do not need to be specified.\n",
    "\n",
    "- Remove indent from all lines\n",
    "- Starts with an alphabet\n",
    "- Ends with `:`\n",
    "\n",
    "- Less stable\n",
    "    - Non-header can be added\n",
    "    - There is no loss of the real headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sections(text, header_indent, unit_indent):\n",
    "    lines = text.split('\\n')\n",
    "    parsed_data = []\n",
    "    current_header = None\n",
    "    current_content = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Remove leading indentation\n",
    "        line = line[header_indent:]\n",
    "\n",
    "        # Check if the line is a header\n",
    "        if line and line[0].isalpha() and line.strip().endswith(':'):\n",
    "            # If there's a current header, save its content before starting a new one\n",
    "            if current_header is not None:\n",
    "                parsed_data.append({'header': current_header, 'content': '\\n'.join(current_content)})\n",
    "                current_content = []\n",
    "            # Set new header\n",
    "            current_header = line.rstrip(':')\n",
    "        else:\n",
    "            if current_header is not None:\n",
    "                # Add line to current content\n",
    "                current_content.append(line[unit_indent:])\n",
    "\n",
    "    # Don't forget to save the last header-content pair\n",
    "    if current_header is not None:\n",
    "        parsed_data.append({'header': current_header, 'content': '\\n'.join(current_content)})\n",
    "\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sin(input, *, out=None) -> Tensor\n",
      "\n",
      "Returns a new tensor with the sine of the elements of :attr:`input`.\n",
      "\n",
      ".. math::\n",
      "    \\text{out}_{i} = \\sin(\\text{input}_{i})\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor.\n",
      "\n",
      "Keyword args:\n",
      "    out (Tensor, optional): the output tensor.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> a = torch.randn(4)\n",
      "    >>> a\n",
      "    tensor([-0.5461,  0.1347, -2.7266, -0.2746])\n",
      "    >>> torch.sin(a)\n",
      "    tensor([-0.5194,  0.1343, -0.4032, -0.2711])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function_doc = getattr(torch, 'sin').__doc__\n",
    "print(function_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applies a 2D convolution over an input signal composed of several input\n",
      "    planes.\n",
      "\n",
      "    In the simplest case, the output value of the layer with input size\n",
      "    :math:`(N, C_{\\text{in}}, H, W)` and output :math:`(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})`\n",
      "    can be precisely described as:\n",
      "\n",
      "    .. math::\n",
      "        \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n",
      "        \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\n",
      "\n",
      "\n",
      "    where :math:`\\star` is the valid 2D `cross-correlation`_ operator,\n",
      "    :math:`N` is a batch size, :math:`C` denotes a number of channels,\n",
      "    :math:`H` is a height of input planes in pixels, and :math:`W` is\n",
      "    width in pixels.\n",
      "    \n",
      "\n",
      "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "    * :attr:`stride` controls the stride for the cross-correlation, a single\n",
      "      number or a tuple.\n",
      "\n",
      "    * :attr:`padding` controls the amount of padding applied to the input. It\n",
      "      can be either a string {'valid', 'same'} or an int / a tuple of ints giving the\n",
      "      amount of implicit padding applied on both sides.\n",
      "\n",
      "    * :attr:`dilation` controls the spacing between the kernel points; also\n",
      "      known as the à trous algorithm. It is harder to describe, but this `link`_\n",
      "      has a nice visualization of what :attr:`dilation` does.\n",
      "\n",
      "    * :attr:`groups` controls the connections between inputs and outputs.\n",
      "      :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n",
      "      :attr:`groups`. For example,\n",
      "\n",
      "        * At groups=1, all inputs are convolved to all outputs.\n",
      "        * At groups=2, the operation becomes equivalent to having two conv\n",
      "          layers side by side, each seeing half the input channels\n",
      "          and producing half the output channels, and both subsequently\n",
      "          concatenated.\n",
      "        * At groups= :attr:`in_channels`, each input channel is convolved with\n",
      "          its own set of filters (of size\n",
      "          :math:`\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}`).\n",
      "\n",
      "    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
      "\n",
      "        - a single ``int`` -- in which case the same value is used for the height and width dimension\n",
      "        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n",
      "          and the second `int` for the width dimension\n",
      "\n",
      "    Note:\n",
      "        When `groups == in_channels` and `out_channels == K * in_channels`,\n",
      "        where `K` is a positive integer, this operation is also known as a \"depthwise convolution\".\n",
      "\n",
      "        In other words, for an input of size :math:`(N, C_{in}, L_{in})`,\n",
      "        a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments\n",
      "        :math:`(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in})`.\n",
      "\n",
      "    Note:\n",
      "        In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
      "\n",
      "    Note:\n",
      "        ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
      "        the input so the output has the shape as the input. However, this mode\n",
      "        doesn't support any stride values other than 1.\n",
      "\n",
      "    Note:\n",
      "        This module supports complex data types i.e. ``complex32, complex64, complex128``.\n",
      "\n",
      "    Args:\n",
      "        in_channels (int): Number of channels in the input image\n",
      "        out_channels (int): Number of channels produced by the convolution\n",
      "        kernel_size (int or tuple): Size of the convolving kernel\n",
      "        stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
      "        padding (int, tuple or str, optional): Padding added to all four sides of\n",
      "            the input. Default: 0\n",
      "        padding_mode (str, optional): ``'zeros'``, ``'reflect'``,\n",
      "            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
      "        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
      "        groups (int, optional): Number of blocked connections from input\n",
      "            channels to output channels. Default: 1\n",
      "        bias (bool, optional): If ``True``, adds a learnable bias to the\n",
      "            output. Default: ``True``\n",
      "    \n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`\n",
      "        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where\n",
      "\n",
      "          .. math::\n",
      "              H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
      "                        \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
      "\n",
      "          .. math::\n",
      "              W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
      "                        \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
      "\n",
      "    Attributes:\n",
      "        weight (Tensor): the learnable weights of the module of shape\n",
      "            :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\n",
      "            :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\n",
      "            The values of these weights are sampled from\n",
      "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "            :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
      "        bias (Tensor):   the learnable bias of the module of shape\n",
      "            (out_channels). If :attr:`bias` is ``True``,\n",
      "            then the values of these weights are\n",
      "            sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "            :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
      "\n",
      "    Examples:\n",
      "\n",
      "        >>> # With square kernels and equal stride\n",
      "        >>> m = nn.Conv2d(16, 33, 3, stride=2)\n",
      "        >>> # non-square kernels and unequal stride and with padding\n",
      "        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
      "        >>> # non-square kernels and unequal stride and with padding and dilation\n",
      "        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
      "        >>> input = torch.randn(20, 16, 50, 100)\n",
      "        >>> output = m(input)\n",
      "\n",
      "    .. _cross-correlation:\n",
      "        https://en.wikipedia.org/wiki/Cross-correlation\n",
      "\n",
      "    .. _link:\n",
      "        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "class_doc = getattr(nn, 'Conv2d').__doc__\n",
    "print(class_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parse_sections() got an unexpected keyword argument 'indent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m parsed_class_doc \u001b[38;5;241m=\u001b[39m \u001b[43mparse_sections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m parsed_class_doc\n",
      "\u001b[0;31mTypeError\u001b[0m: parse_sections() got an unexpected keyword argument 'indent'"
     ]
    }
   ],
   "source": [
    "parsed_class_doc = parse_sections(class_doc, indent=4)\n",
    "parsed_class_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sin(input, *, out=None) -> Tensor\n",
      "\n",
      "Returns a new tensor with the sine of the elements of :attr:`input`.\n",
      "\n",
      ".. math::\n",
      "    \\text{out}_{i} = \\sin(\\text{input}_{i})\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor.\n",
      "\n",
      "Keyword args:\n",
      "    out (Tensor, optional): the output tensor.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> a = torch.randn(4)\n",
      "    >>> a\n",
      "    tensor([-0.5461,  0.1347, -2.7266, -0.2746])\n",
      "    >>> torch.sin(a)\n",
      "    tensor([-0.5194,  0.1343, -0.4032, -0.2711])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(function_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note :\n",
      "When `groups == in_channels` and `out_channels == K * in_channels`,\n",
      "where `K` is a positive integer, this operation is also known as a \"depthwise convolution\".\n",
      "\n",
      "In other words, for an input of size :math:`(N, C_{in}, L_{in})`,\n",
      "a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments\n",
      ":math:`(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in})`.\n",
      " \n",
      "\n",
      "Note :\n",
      "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
      " \n",
      "\n",
      "Note :\n",
      "``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
      "the input so the output has the shape as the input. However, this mode\n",
      "doesn't support any stride values other than 1.\n",
      " \n",
      "\n",
      "Note :\n",
      "This module supports complex data types i.e. ``complex32, complex64, complex128``.\n",
      " \n",
      "\n",
      "Args :\n",
      "in_channels (int): Number of channels in the input image\n",
      "out_channels (int): Number of channels produced by the convolution\n",
      "kernel_size (int or tuple): Size of the convolving kernel\n",
      "stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
      "padding (int, tuple or str, optional): Padding added to all four sides of\n",
      "    the input. Default: 0\n",
      "padding_mode (str, optional): ``'zeros'``, ``'reflect'``,\n",
      "    ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
      "dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
      "groups (int, optional): Number of blocked connections from input\n",
      "    channels to output channels. Default: 1\n",
      "bias (bool, optional): If ``True``, adds a learnable bias to the\n",
      "    output. Default: ``True``\n",
      "\n",
      " \n",
      "\n",
      "Shape :\n",
      "- Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`\n",
      "- Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where\n",
      "\n",
      "  .. math::\n",
      "      H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
      "                \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
      "\n",
      "  .. math::\n",
      "      W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
      "                \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
      " \n",
      "\n",
      "Attributes :\n",
      "weight (Tensor): the learnable weights of the module of shape\n",
      "    :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\n",
      "    :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\n",
      "    The values of these weights are sampled from\n",
      "    :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "    :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
      "bias (Tensor):   the learnable bias of the module of shape\n",
      "    (out_channels). If :attr:`bias` is ``True``,\n",
      "    then the values of these weights are\n",
      "    sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "    :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
      " \n",
      "\n",
      "Examples :\n",
      "\n",
      ">>> # With square kernels and equal stride\n",
      ">>> m = nn.Conv2d(16, 33, 3, stride=2)\n",
      ">>> # non-square kernels and unequal stride and with padding\n",
      ">>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
      ">>> # non-square kernels and unequal stride and with padding and dilation\n",
      ">>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
      ">>> input = torch.randn(20, 16, 50, 100)\n",
      ">>> output = m(input)\n",
      "\n",
      "cross-correlation:\n",
      "https://en.wikipedia.org/wiki/Cross-correlation\n",
      "\n",
      "link:\n",
      "https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_class_doc = parse_sections(class_doc, header_indent=4, unit_indent=4)\n",
    "\n",
    "started_header = False\n",
    "for session in parsed_class_doc:\n",
    "    if \"Note\" in session['header']:\n",
    "        started_header = True\n",
    "    if started_header:\n",
    "        print(session['header'], ':')\n",
    "        print(session['content'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args :\n",
      "input (Tensor): the input tensor.\n",
      " \n",
      "\n",
      "Keyword args :\n",
      "out (Tensor, optional): the output tensor.\n",
      " \n",
      "\n",
      "Example :\n",
      "\n",
      ">>> a = torch.randn(4)\n",
      ">>> a\n",
      "tensor([-0.5461,  0.1347, -2.7266, -0.2746])\n",
      ">>> torch.sin(a)\n",
      "tensor([-0.5194,  0.1343, -0.4032, -0.2711])\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_function_doc = parse_sections(function_doc, header_indent=0, unit_indent=4)\n",
    "for session in parsed_function_doc:\n",
    "    print(session['header'], ':')\n",
    "    print(session['content'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Select the Implementation2 for the following reasons\n",
    "\n",
    "- for the sake of RAG, jsonl structure has its advantage\n",
    "- we are free from the header specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Converter Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase1 : Constructor Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args :\n",
      "in_features: size of each input sample\n",
      "out_features: size of each output sample\n",
      "bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "    Default: ``True``\n",
      " \n",
      "\n",
      "Shape :\n",
      "- Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "  dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "- Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "  are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      " \n",
      "\n",
      "Attributes :\n",
      "weight: the learnable weights of the module of shape\n",
      "    :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "    initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "    :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "        If :attr:`bias` is ``True``, the values are initialized from\n",
      "        :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      " \n",
      "\n",
      "Examples :\n",
      "\n",
      ">>> m = nn.Linear(20, 30)\n",
      ">>> input = torch.randn(128, 20)\n",
      ">>> output = m(input)\n",
      ">>> print(output.size())\n",
      "torch.Size([128, 30])\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_doc = getattr(nn, 'Linear').__doc__\n",
    "parsed_class_doc = parse_sections(class_doc, header_indent=4, unit_indent=4)\n",
    "for session in parsed_class_doc:\n",
    "    print(session['header'], ':')\n",
    "    print(session['content'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'},\n",
       " {'header': 'Keyword args',\n",
       "  'content': 'out (Tensor, optional): the output tensor.\\n'},\n",
       " {'header': 'Example',\n",
       "  'content': '\\n>>> a = torch.randn(4)\\n>>> a\\ntensor([-0.5461,  0.1347, -2.7266, -0.2746])\\n>>> torch.sin(a)\\ntensor([-0.5194,  0.1343, -0.4032, -0.2711])\\n'}]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_function_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = torch.nn.Conv2d(\n",
    "    in_channels=1, \n",
    "    out_channels=2, \n",
    "    kernel_size=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsin(input, *, out=None) -> Tensor\\n\\nReturns a new tensor with the sine of the elements of :attr:`input`.\\n\\n.. math::\\n    \\\\text{out}_{i} = \\\\sin(\\\\text{input}_{i})\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.randn(4)\\n    >>> a\\n    tensor([-0.5461,  0.1347, -2.7266, -0.2746])\\n    >>> torch.sin(a)\\n    tensor([-0.5194,  0.1343, -0.4032, -0.2711])\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase1\n",
    "\n",
    "# Prompt Template Blue Print\n",
    "# Not just the exact target, but the context\n",
    "# We'd better start with the smallest example\n",
    "\n",
    "doc = class_doc\n",
    "target = 'torch.nn.Linear'\n",
    "\n",
    "phase1_example = f'''\n",
    "Generate the code lines to initiate an instance of the target module.\n",
    "\n",
    "target: torch.nn.Conv2d\n",
    "\n",
    "output:\n",
    "torch_model = torch.nn.Conv2d(\n",
    "    in_channels=10, \n",
    "    out_channels=20, \n",
    "    kernel_size=3\n",
    ")\n",
    "'''\n",
    "\n",
    "phase1_prompt = f'''\n",
    "\n",
    "<<Instruction>>\n",
    "\n",
    "Given the context, complete the <<Task>>.\n",
    "\n",
    "<<Documentation for {target}>>\n",
    "\n",
    "{doc}\n",
    "\n",
    "<<Example>>\n",
    "{phase1_example}\n",
    "\n",
    "<<Task>>\n",
    "\n",
    "Write the code lines to initiate an instance of the target module in python code block.\n",
    "\n",
    "target: {target}\n",
    "\n",
    "output:\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<<Instruction>>\n",
      "\n",
      "Given the context, complete the <<Task>>.\n",
      "\n",
      "<<Documentation for torch.nn.Linear>>\n",
      "\n",
      "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
      "\n",
      "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "    Args:\n",
      "        in_features: size of each input sample\n",
      "        out_features: size of each output sample\n",
      "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "            Default: ``True``\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "          dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "        - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "\n",
      "    Attributes:\n",
      "        weight: the learnable weights of the module of shape\n",
      "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "                If :attr:`bias` is ``True``, the values are initialized from\n",
      "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> m = nn.Linear(20, 30)\n",
      "        >>> input = torch.randn(128, 20)\n",
      "        >>> output = m(input)\n",
      "        >>> print(output.size())\n",
      "        torch.Size([128, 30])\n",
      "    \n",
      "\n",
      "<<Example>>\n",
      "\n",
      "Generate the code lines to initiate an instance of the target module.\n",
      "\n",
      "target: torch.nn.Conv2d\n",
      "\n",
      "output:\n",
      "torch_model = torch.nn.Conv2d(\n",
      "    in_channels=1, \n",
      "    out_channels=2, \n",
      "    kernel_size=3\n",
      ")\n",
      "\n",
      "\n",
      "<<Task>>\n",
      "\n",
      "Write the code lines to initiate an instance of the target module in python code block.\n",
      "\n",
      "target: torch.nn.Linear\n",
      "\n",
      "output:\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(phase1_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2\n",
    "\n",
    "# Check if an instance is initiated by the generated text\n",
    "# If the test fails, return to Phase1\n",
    "# If passes, do the next\n",
    "\n",
    "# Phase1\n",
    "\n",
    "# Prompt Template Blue Print\n",
    "# Not just the exact target, but the context\n",
    "# We'd better start with the smallest example\n",
    "\n",
    "doc = class_doc\n",
    "target = 'torch.nn.Linear'\n",
    "\n",
    "phase1_output = \"\"\"\n",
    "torch_model = torch.nn.Linear(\n",
    "    in_features=32, \n",
    "    out_features=64, \n",
    "    bias=True\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "phase2_example = f'''\n",
    "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
    "\n",
    "target: \n",
    "torch_model = torch.nn.Conv2d(\n",
    "    in_channels=10, \n",
    "    out_channels=20, \n",
    "    kernel_size=3\n",
    ")\n",
    "\n",
    "output:\n",
    "\n",
    "```python\n",
    "inputs = [torch.randn(1, 10, 32, 32)]\n",
    "output = torch_model.forward(*inputs)\n",
    "\n",
    "```\n",
    "\n",
    "'''\n",
    "\n",
    "phase2_prompt = f'''\n",
    "\n",
    "<<Instruction>>\n",
    "\n",
    "Given the context, complete the <<Task>>.\n",
    "\n",
    "<<Documentation for {target}>>\n",
    "\n",
    "{doc}\n",
    "\n",
    "<<Example>>\n",
    "{phase2_example}\n",
    "\n",
    "<<Task>>\n",
    "\n",
    "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
    "\n",
    "target: {phase1_output}\n",
    "\n",
    "output:\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<<Instruction>>\n",
      "\n",
      "Given the context, complete the <<Task>>.\n",
      "\n",
      "<<Documentation for torch.nn.Linear>>\n",
      "\n",
      "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
      "\n",
      "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "    Args:\n",
      "        in_features: size of each input sample\n",
      "        out_features: size of each output sample\n",
      "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "            Default: ``True``\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "          dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "        - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "\n",
      "    Attributes:\n",
      "        weight: the learnable weights of the module of shape\n",
      "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "                If :attr:`bias` is ``True``, the values are initialized from\n",
      "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> m = nn.Linear(20, 30)\n",
      "        >>> input = torch.randn(128, 20)\n",
      "        >>> output = m(input)\n",
      "        >>> print(output.size())\n",
      "        torch.Size([128, 30])\n",
      "    \n",
      "\n",
      "<<Example>>\n",
      "\n",
      "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
      "\n",
      "target: \n",
      "torch_model = torch.nn.Conv2d(\n",
      "    in_channels=10, \n",
      "    out_channels=20, \n",
      "    kernel_size=3\n",
      ")\n",
      "\n",
      "output:\n",
      "\n",
      "```python\n",
      "inputs = [torch.randn(1, 10, 32, 32)]\n",
      "output = torch_model.forward(*inputs)\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "<<Task>>\n",
      "\n",
      "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
      "\n",
      "target: \n",
      "torch_model = torch.nn.Linear(\n",
      "    in_features=32, \n",
      "    out_features=64, \n",
      "    bias=True\n",
      ")\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(phase2_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch_model = torch.nn.Linear(\n",
      "    in_features=32, \n",
      "    out_features=64, \n",
      "    bias=True\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(phase1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = torch.nn.Linear(\n",
    "    in_features=32, \n",
    "    out_features=64, \n",
    "    bias=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.randn(128, 32)]\n",
    "output = torch_model.forward(*inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phase2_prompt(phase1_output):\n",
    "    return f'''\n",
    "\n",
    "<<Instruction>>\n",
    "\n",
    "Given the context, complete the <<Task>>.\n",
    "\n",
    "<<Documentation for {target}>>\n",
    "\n",
    "{doc}\n",
    "\n",
    "<<Example>>\n",
    "{example}\n",
    "\n",
    "<<Task>>\n",
    "\n",
    "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
    "\n",
    "target: {phase1_output}\n",
    "\n",
    "output:\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<<Instruction>>\n",
      "\n",
      "Given the context, complete the <<Task>>.\n",
      "\n",
      "<<Documentation for torch.nn.Linear>>\n",
      "\n",
      "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
      "\n",
      "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "    Args:\n",
      "        in_features: size of each input sample\n",
      "        out_features: size of each output sample\n",
      "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "            Default: ``True``\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "          dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "        - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "\n",
      "    Attributes:\n",
      "        weight: the learnable weights of the module of shape\n",
      "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "                If :attr:`bias` is ``True``, the values are initialized from\n",
      "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> m = nn.Linear(20, 30)\n",
      "        >>> input = torch.randn(128, 20)\n",
      "        >>> output = m(input)\n",
      "        >>> print(output.size())\n",
      "        torch.Size([128, 30])\n",
      "    \n",
      "\n",
      "<<Example>>\n",
      "\n",
      "Generate the code lines to initiate an instance of the target module.\n",
      "\n",
      "target: torch.nn.Conv2d\n",
      "\n",
      "output:\n",
      "torch_model = torch.nn.Conv2d(\n",
      "    in_channels=1, \n",
      "    out_channels=2, \n",
      "    kernel_size=3\n",
      ")\n",
      "\n",
      "\n",
      "<<Task>>\n",
      "\n",
      "Generate the code lines to initiate an instance of the target module.\n",
      "\n",
      "target: torch.nn.Linear\n",
      "\n",
      "output:\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(phase1_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Phase1\n",
    "\n",
    "```python\n",
    "torch_model = torch.nn.Linear(\n",
    "    in_features=20, \n",
    "    out_features=30, \n",
    "    bias=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = torch.nn.Linear(\n",
    "    in_features=20, \n",
    "    out_features=30, \n",
    "    bias=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<<Instruction>>\n",
      "\n",
      "Given the context, complete the <<Task>>.\n",
      "\n",
      "<<Documentation for torch.nn.Linear>>\n",
      "\n",
      "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
      "\n",
      "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "    Args:\n",
      "        in_features: size of each input sample\n",
      "        out_features: size of each output sample\n",
      "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "            Default: ``True``\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "          dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "        - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "\n",
      "    Attributes:\n",
      "        weight: the learnable weights of the module of shape\n",
      "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "                If :attr:`bias` is ``True``, the values are initialized from\n",
      "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> m = nn.Linear(20, 30)\n",
      "        >>> input = torch.randn(128, 20)\n",
      "        >>> output = m(input)\n",
      "        >>> print(output.size())\n",
      "        torch.Size([128, 30])\n",
      "    \n",
      "\n",
      "<<Example>>\n",
      "\n",
      "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
      "\n",
      "target: \n",
      "torch_model = torch.nn.Conv2d(\n",
      "    in_channels=10, \n",
      "    out_channels=20, \n",
      "    kernel_size=3\n",
      ")\n",
      "\n",
      "output:\n",
      "\n",
      "```python\n",
      "inputs = [torch.randn(1, 10, 32, 32)]\n",
      "output = torch_model.forward(*inputs)\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "<<Task>>\n",
      "\n",
      "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
      "\n",
      "target: \n",
      "torch_model = torch.nn.Linear(\n",
      "    in_features=20, \n",
      "    out_features=30, \n",
      "    bias=True\n",
      ")\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_phase2_prompt('''\n",
    "torch_model = torch.nn.Linear(\n",
    "    in_features=20, \n",
    "    out_features=30, \n",
    "    bias=True\n",
    ")\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Phase2\n",
    "\n",
    "```python\n",
    "inputs = [torch.randn(128, 20)]\n",
    "output = torch_model.forward(*inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.randn(128, 20)]\n",
    "output = torch_model.forward(*inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch_model = torch.nn.Linear(\n",
    "    in_features=20, \n",
    "    out_features=30\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = torch.nn.Linear(\n",
    "    in_features=20, \n",
    "    out_features=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<<Instruction>>\n",
      "\n",
      "Given the context, complete the <<Task>>.\n",
      "\n",
      "<<Documentation for torch.nn.Linear>>\n",
      "\n",
      "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
      "\n",
      "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "    Args:\n",
      "        in_features: size of each input sample\n",
      "        out_features: size of each output sample\n",
      "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "            Default: ``True``\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "          dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "        - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "\n",
      "    Attributes:\n",
      "        weight: the learnable weights of the module of shape\n",
      "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "                If :attr:`bias` is ``True``, the values are initialized from\n",
      "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> m = nn.Linear(20, 30)\n",
      "        >>> input = torch.randn(128, 20)\n",
      "        >>> output = m(input)\n",
      "        >>> print(output.size())\n",
      "        torch.Size([128, 30])\n",
      "    \n",
      "\n",
      "<<Example>>\n",
      "\n",
      "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
      "\n",
      "target: \n",
      "torch_model = torch.nn.Conv2d(\n",
      "    in_channels=10, \n",
      "    out_channels=20, \n",
      "    kernel_size=3\n",
      ")\n",
      "\n",
      "output:\n",
      "\n",
      "```python\n",
      "inputs = [torch.randn(1, 10, 32, 32)]\n",
      "output = torch_model.forward(*inputs)\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "<<Task>>\n",
      "\n",
      "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
      "\n",
      "target: \n",
      "torch_model = torch.nn.Linear(\n",
      "    in_features=20, \n",
      "    out_features=30\n",
      ")\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_phase2_prompt('''\n",
    "torch_model = torch.nn.Linear(\n",
    "    in_features=20, \n",
    "    out_features=30\n",
    ")\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.randn(128, 20)]\n",
    "output = torch_model.forward(*inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT3.5 fhgenie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch_model = torch.nn.Linear(\n",
    "    in_features=10,\n",
    "    out_features=5,\n",
    "    bias=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = torch.nn.Linear(\n",
    "    in_features=10,\n",
    "    out_features=5,\n",
    "    bias=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<<Instruction>>\n",
      "\n",
      "Given the context, complete the <<Task>>.\n",
      "\n",
      "<<Documentation for torch.nn.Linear>>\n",
      "\n",
      "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
      "\n",
      "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "    Args:\n",
      "        in_features: size of each input sample\n",
      "        out_features: size of each output sample\n",
      "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "            Default: ``True``\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "          dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "        - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "\n",
      "    Attributes:\n",
      "        weight: the learnable weights of the module of shape\n",
      "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "                If :attr:`bias` is ``True``, the values are initialized from\n",
      "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> m = nn.Linear(20, 30)\n",
      "        >>> input = torch.randn(128, 20)\n",
      "        >>> output = m(input)\n",
      "        >>> print(output.size())\n",
      "        torch.Size([128, 30])\n",
      "    \n",
      "\n",
      "<<Example>>\n",
      "\n",
      "Generate the code lines to initiate an instance of the target module.\n",
      "\n",
      "target: torch.nn.Conv2d\n",
      "\n",
      "output:\n",
      "torch_model = torch.nn.Conv2d(\n",
      "    in_channels=1, \n",
      "    out_channels=2, \n",
      "    kernel_size=3\n",
      ")\n",
      "\n",
      "\n",
      "<<Task>>\n",
      "\n",
      "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
      "\n",
      "target: \n",
      "torch_model = torch.nn.Linear(\n",
      "    in_features=10,\n",
      "    out_features=5,\n",
      "    bias=True\n",
      ")\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_phase2_prompt('''\n",
    "torch_model = torch.nn.Linear(\n",
    "    in_features=10,\n",
    "    out_features=5,\n",
    "    bias=True\n",
    ")\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input = torch.randn(32, 10)  # Example input\n",
    "output = torch_model(input)  # Forward pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class target is assumed\n",
    "\n",
    "# The target to be converted\n",
    "constructor = 'torch.nn.Linear'\n",
    "\n",
    "# The function name representing the functions properly. Ex) log, do_something.\n",
    "# Option1 : Use the name of the first function. (preferred)\n",
    "# Option2 : Ask AI.\n",
    "# No matter what we choose, we must refine them.\n",
    "class_name = \"linear_layer\"\n",
    "\n",
    "phase1_output = \"\"\"\n",
    "torch_model = torch.nn.Linear(\n",
    "    in_features=10,\n",
    "    out_features=5,\n",
    "    bias=True\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Inputs as a list generated by AI.\n",
    "# It will be unpacked to be passed.\n",
    "# Would kwargs be better than args? It would be nice to try both.\n",
    "inputs = 'torch.randn(32, 10)'\n",
    "\n",
    "# Another inputs are required to generate the target instance.\n",
    "# Try kwargs here.\n",
    "construction_args = r\"{'in_channels':10, 'out_channels':10, 'kernel_size'=1}\"\n",
    "\n",
    "template = f\"\"\"\n",
    "def test_{class_name}_converter(self):\n",
    "    # Initialize the model directly from its constructor\n",
    "    {phase1_output}\n",
    "    torch_model.eval()\n",
    "\t# Initialize the model and input tensor\n",
    "    inputs = {inputs}\n",
    "\n",
    "    # Convert the model and ensure the HTML trace is saved\n",
    "    keras_model = nobuco.pytorch_to_keras(\n",
    "        torch_model,\n",
    "        args=[*inputs], kwargs=None,\n",
    "        inputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
    "        outputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
    "        save_trace_html=True\n",
    "    )\n",
    "\n",
    "    # Read the contents of the trace.html file\n",
    "    with open('trace.html', 'r', encoding='utf-8') as file:\n",
    "        trace_html = file.read()\n",
    "\n",
    "    # Assertions for the content of trace_html\n",
    "    self.assertNotIn('Max diff', trace_html, \"The trace HTML should not contain 'Max diff'\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unittest_template(class_name, phase1_output, inputs):\n",
    "    return f\"\"\"\n",
    "def test_{class_name}_converter(self):\n",
    "    # Initialize the model directly from its constructor\n",
    "    {phase1_output}\n",
    "    torch_model.eval()\n",
    "\t# Initialize the model and input tensor\n",
    "    {inputs}\n",
    "\n",
    "    # Convert the model and ensure the HTML trace is saved\n",
    "    keras_model = nobuco.pytorch_to_keras(\n",
    "        torch_model,\n",
    "        args=[*inputs], kwargs=None,\n",
    "        inputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
    "        outputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
    "        save_trace_html=True\n",
    "    )\n",
    "\n",
    "    # Read the contents of the trace.html file\n",
    "    with open('trace.html', 'r', encoding='utf-8') as file:\n",
    "        trace_html = file.read()\n",
    "\n",
    "    # Assertions for the content of trace_html\n",
    "    self.assertNotIn('Max diff', trace_html, \"The trace HTML should not contain 'Max diff'\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def test_linear_layer_converter(self):\n",
      "    # Initialize the model directly from its constructor\n",
      "    \n",
      "torch_model = torch.nn.Linear(\n",
      "    in_features=10,\n",
      "    out_features=5,\n",
      "    bias=True\n",
      ")\n",
      "\n",
      "    torch_model.eval()\n",
      "\t# Initialize the model and input tensor\n",
      "    inputs = torch.randn(32, 10)\n",
      "\n",
      "    # Convert the model and ensure the HTML trace is saved\n",
      "    keras_model = nobuco.pytorch_to_keras(\n",
      "        torch_model,\n",
      "        args=[*inputs], kwargs=None,\n",
      "        inputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
      "        outputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
      "        save_trace_html=True\n",
      "    )\n",
      "\n",
      "    # Read the contents of the trace.html file\n",
      "    with open('trace.html', 'r', encoding='utf-8') as file:\n",
      "        trace_html = file.read()\n",
      "\n",
      "    # Assertions for the content of trace_html\n",
      "    self.assertNotIn('Max diff', trace_html, \"The trace HTML should not contain 'Max diff'\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phase1_prompt(target, doc, example):\n",
    "    return f'''\n",
    "\n",
    "<<Instruction>>\n",
    "\n",
    "Given the context, complete the <<Task>>.\n",
    "\n",
    "<<Documentation for {target}>>\n",
    "\n",
    "{doc}\n",
    "\n",
    "<<Example>>\n",
    "{example}\n",
    "\n",
    "<<Task>>\n",
    "\n",
    "Write the code lines to initiate an instance of the target module in python code block.\n",
    "\n",
    "target: {target}\n",
    "\n",
    "output:\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phase2_prompt(target, doc, example, phase1_output):\n",
    "    return f'''\n",
    "\n",
    "<<Instruction>>\n",
    "\n",
    "Given the context, complete the <<Task>>.\n",
    "\n",
    "<<Documentation for {target}>>\n",
    "\n",
    "{doc}\n",
    "\n",
    "<<Example>>\n",
    "{example}\n",
    "\n",
    "<<Task>>\n",
    "\n",
    "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
    "\n",
    "target: {phase1_output}\n",
    "\n",
    "output:\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"nn.modules.activation.MultiheadAttention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_doc = nn.modules.activation.MultiheadAttention.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<<Instruction>>\n",
      "\n",
      "Given the context, complete the <<Task>>.\n",
      "\n",
      "<<Documentation for nn.modules.activation.MultiheadAttention>>\n",
      "\n",
      "Allows the model to jointly attend to information\n",
      "    from different representation subspaces as described in the paper:\n",
      "    `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\n",
      "\n",
      "    Multi-Head Attention is defined as:\n",
      "\n",
      "    .. math::\n",
      "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
      "\n",
      "    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
      "\n",
      "    ``nn.MultiHeadAttention`` will use the optimized implementations of\n",
      "    ``scaled_dot_product_attention()`` when possible.\n",
      "\n",
      "    In addition to support for the new ``scaled_dot_product_attention()``\n",
      "    function, for speeding up Inference, MHA will use\n",
      "    fastpath inference with support for Nested Tensors, iff:\n",
      "\n",
      "    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).\n",
      "    - inputs are batched (3D) with ``batch_first==True``\n",
      "    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n",
      "    - training is disabled (using ``.eval()``)\n",
      "    - ``add_bias_kv`` is ``False``\n",
      "    - ``add_zero_attn`` is ``False``\n",
      "    - ``batch_first`` is ``True`` and the input is batched\n",
      "    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\n",
      "    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n",
      "      nor ``attn_mask`` is passed\n",
      "    - autocast is disabled\n",
      "\n",
      "    If the optimized inference fastpath implementation is in use, a\n",
      "    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n",
      "    ``query``/``key``/``value`` to represent padding more efficiently than using a\n",
      "    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n",
      "    will be returned, and an additional speedup proportional to the fraction of the input\n",
      "    that is padding can be expected.\n",
      "\n",
      "    Args:\n",
      "        embed_dim: Total dimension of the model.\n",
      "        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n",
      "            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n",
      "        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n",
      "        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n",
      "        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n",
      "        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n",
      "            Default: ``False``.\n",
      "        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n",
      "        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n",
      "        batch_first: If ``True``, then the input and output tensors are provided\n",
      "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> # xdoctest: +SKIP\n",
      "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
      "\n",
      "    .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n",
      "         https://arxiv.org/abs/2205.14135\n",
      "\n",
      "    \n",
      "\n",
      "<<Example>>\n",
      "\n",
      "Generate the code lines to initiate an instance of the target module.\n",
      "\n",
      "target: torch.nn.Conv2d\n",
      "\n",
      "output:\n",
      "torch_model = torch.nn.Conv2d(\n",
      "    in_channels=10, \n",
      "    out_channels=20, \n",
      "    kernel_size=3\n",
      ")\n",
      "\n",
      "\n",
      "<<Task>>\n",
      "\n",
      "Write the code lines to initiate an instance of the target module in python code block.\n",
      "\n",
      "target: nn.modules.activation.MultiheadAttention\n",
      "\n",
      "output:\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_phase1_prompt(target, class_doc, phase1_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch_model = nn.modules.activation.MultiheadAttention(\n",
    "    embed_dim=256,\n",
    "    num_heads=8,\n",
    "    dropout=0.1,\n",
    "    bias=True,\n",
    "    add_bias_kv=False,\n",
    "    add_zero_attn=False,\n",
    "    kdim=None,\n",
    "    vdim=None,\n",
    "    batch_first=False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = nn.modules.activation.MultiheadAttention(\n",
    "    embed_dim=256,\n",
    "    num_heads=8,\n",
    "    dropout=0.1,\n",
    "    bias=True,\n",
    "    add_bias_kv=False,\n",
    "    add_zero_attn=False,\n",
    "    kdim=None,\n",
    "    vdim=None,\n",
    "    batch_first=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1_output = \"\"\"\n",
    "torch_model = nn.modules.activation.MultiheadAttention(\n",
    "    embed_dim=256,\n",
    "    num_heads=8,\n",
    "    dropout=0.1,\n",
    "    bias=True,\n",
    "    add_bias_kv=False,\n",
    "    add_zero_attn=False,\n",
    "    kdim=None,\n",
    "    vdim=None,\n",
    "    batch_first=False\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<<Instruction>>\n",
      "\n",
      "Given the context, complete the <<Task>>.\n",
      "\n",
      "<<Documentation for nn.modules.activation.MultiheadAttention>>\n",
      "\n",
      "Allows the model to jointly attend to information\n",
      "    from different representation subspaces as described in the paper:\n",
      "    `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\n",
      "\n",
      "    Multi-Head Attention is defined as:\n",
      "\n",
      "    .. math::\n",
      "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
      "\n",
      "    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
      "\n",
      "    ``nn.MultiHeadAttention`` will use the optimized implementations of\n",
      "    ``scaled_dot_product_attention()`` when possible.\n",
      "\n",
      "    In addition to support for the new ``scaled_dot_product_attention()``\n",
      "    function, for speeding up Inference, MHA will use\n",
      "    fastpath inference with support for Nested Tensors, iff:\n",
      "\n",
      "    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).\n",
      "    - inputs are batched (3D) with ``batch_first==True``\n",
      "    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n",
      "    - training is disabled (using ``.eval()``)\n",
      "    - ``add_bias_kv`` is ``False``\n",
      "    - ``add_zero_attn`` is ``False``\n",
      "    - ``batch_first`` is ``True`` and the input is batched\n",
      "    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\n",
      "    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n",
      "      nor ``attn_mask`` is passed\n",
      "    - autocast is disabled\n",
      "\n",
      "    If the optimized inference fastpath implementation is in use, a\n",
      "    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n",
      "    ``query``/``key``/``value`` to represent padding more efficiently than using a\n",
      "    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n",
      "    will be returned, and an additional speedup proportional to the fraction of the input\n",
      "    that is padding can be expected.\n",
      "\n",
      "    Args:\n",
      "        embed_dim: Total dimension of the model.\n",
      "        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n",
      "            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n",
      "        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n",
      "        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n",
      "        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n",
      "        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n",
      "            Default: ``False``.\n",
      "        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n",
      "        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n",
      "        batch_first: If ``True``, then the input and output tensors are provided\n",
      "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> # xdoctest: +SKIP\n",
      "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
      "\n",
      "    .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n",
      "         https://arxiv.org/abs/2205.14135\n",
      "\n",
      "    \n",
      "\n",
      "<<Example>>\n",
      "\n",
      "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
      "\n",
      "target: \n",
      "torch_model = torch.nn.Conv2d(\n",
      "    in_channels=10, \n",
      "    out_channels=20, \n",
      "    kernel_size=3\n",
      ")\n",
      "\n",
      "output:\n",
      "\n",
      "```python\n",
      "inputs = [torch.randn(1, 10, 32, 32)]\n",
      "output = torch_model.forward(*inputs)\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "<<Task>>\n",
      "\n",
      "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
      "\n",
      "target: \n",
      "torch_model = nn.modules.activation.MultiheadAttention(\n",
      "    embed_dim=256,\n",
      "    num_heads=8,\n",
      "    dropout=0.1,\n",
      "    bias=True,\n",
      "    add_bias_kv=False,\n",
      "    add_zero_attn=False,\n",
      "    kdim=None,\n",
      "    vdim=None,\n",
      "    batch_first=False\n",
      ")\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_phase2_prompt(target, class_doc, phase2_example, phase1_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "query = torch.randn(32, 10, 64)  # Example query tensor\n",
    "key = torch.randn(32, 20, 64)  # Example key tensor\n",
    "value = torch.randn(32, 20, 64)  # Example value tensor\n",
    "output, attn_output_weights = torch_model(query, key, value)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "was expecting embedding dimension of 256, but got 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[210], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m key \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m64\u001b[39m)  \u001b[38;5;66;03m# Example key tensor\u001b[39;00m\n\u001b[1;32m     15\u001b[0m value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m64\u001b[39m)  \u001b[38;5;66;03m# Example value tensor\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:5280\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5275\u001b[0m         \u001b[38;5;66;03m# We have the attn_mask, and use that to merge kpm into it.\u001b[39;00m\n\u001b[1;32m   5276\u001b[0m         \u001b[38;5;66;03m# Turn off use of is_causal hint, as the merged mask is no\u001b[39;00m\n\u001b[1;32m   5277\u001b[0m         \u001b[38;5;66;03m# longer causal.\u001b[39;00m\n\u001b[1;32m   5278\u001b[0m         is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 5280\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m embed_dim \u001b[38;5;241m==\u001b[39m embed_dim_to_check, \\\n\u001b[1;32m   5281\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas expecting embedding dimension of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_dim, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m   5283\u001b[0m     \u001b[38;5;66;03m# embed_dim can be a tensor when JIT tracing\u001b[39;00m\n\u001b[1;32m   5284\u001b[0m     head_dim \u001b[38;5;241m=\u001b[39m embed_dim\u001b[38;5;241m.\u001b[39mdiv(num_heads, rounding_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrunc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: was expecting embedding dimension of 256, but got 64"
     ]
    }
   ],
   "source": [
    "torch_model = nn.modules.activation.MultiheadAttention(\n",
    "    embed_dim=256,\n",
    "    num_heads=8,\n",
    "    dropout=0.1,\n",
    "    bias=True,\n",
    "    add_bias_kv=False,\n",
    "    add_zero_attn=False,\n",
    "    kdim=None,\n",
    "    vdim=None,\n",
    "    batch_first=False\n",
    ")\n",
    "\n",
    "query = torch.randn(32, 10, 64)  # Example query tensor\n",
    "key = torch.randn(32, 20, 64)  # Example key tensor\n",
    "value = torch.randn(32, 20, 64)  # Example value tensor\n",
    "output, attn_output_weights = torch_model(query, key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugging_template = \"\"\"\n",
    "\n",
    "<<Executed Code>>\n",
    "\n",
    "torch_model = nn.modules.activation.MultiheadAttention(\n",
    "    embed_dim=256,\n",
    "    num_heads=8,\n",
    "    dropout=0.1,\n",
    "    bias=True,\n",
    "    add_bias_kv=False,\n",
    "    add_zero_attn=False,\n",
    "    kdim=None,\n",
    "    vdim=None,\n",
    "    batch_first=False\n",
    ")\n",
    "\n",
    "query = torch.randn(32, 10, 64)  # Example query tensor\n",
    "key = torch.randn(32, 20, 64)  # Example key tensor\n",
    "value = torch.randn(32, 20, 64)  # Example value tensor\n",
    "output, attn_output_weights = torch_model(query, key, value)\n",
    "\n",
    "<<Error>>\n",
    "\n",
    "File ~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241, in MultiheadAttention.forward(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\n",
    "   1227     attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "   1228         query, key, value, self.embed_dim, self.num_heads,\n",
    "   1229         self.in_proj_weight, self.in_proj_bias,\n",
    "   (...)\n",
    "   1238         average_attn_weights=average_attn_weights,\n",
    "   1239         is_causal=is_causal)\n",
    "   1240 else:\n",
    "-> 1241     attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "   1242         query, key, value, self.embed_dim, self.num_heads,\n",
    "   1243         self.in_proj_weight, self.in_proj_bias,\n",
    "   1244         self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "   1245         self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "   1246         training=self.training,\n",
    "   1247         key_padding_mask=key_padding_mask,\n",
    "   1248         need_weights=need_weights,\n",
    "   1249         attn_mask=attn_mask,\n",
    "   1250         average_attn_weights=average_attn_weights,\n",
    "   1251         is_causal=is_causal)\n",
    "   1252 if self.batch_first and is_batched:\n",
    "   1253     return attn_output.transpose(1, 0), attn_output_weights\n",
    "\n",
    "File ~/.local/lib/python3.10/site-packages/torch/nn/functional.py:5280, in multi_head_attention_forward(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\n",
    "   5274     if key_padding_mask is not None:\n",
    "   5275         # We have the attn_mask, and use that to merge kpm into it.\n",
    "   5276         # Turn off use of is_causal hint, as the merged mask is no\n",
    "   5277         # longer causal.\n",
    "   5278         is_causal = False\n",
    "-> 5280 assert embed_dim == embed_dim_to_check, \\\n",
    "   5281     f\"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}\"\n",
    "   5282 if isinstance(embed_dim, torch.Tensor):\n",
    "   5283     # embed_dim can be a tensor when JIT tracing\n",
    "   5284     head_dim = embed_dim.div(num_heads, rounding_mode='trunc')\n",
    "\n",
    "AssertionError: was expecting embedding dimension of 256, but got 64\n",
    "\n",
    "<<Task>>\n",
    "\n",
    "Given the error, rewrite the code in python code block.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.randn(32, 10, 64)  # Example query tensor\n",
    "key = torch.randn(32, 20, 64)  # Example key tensor\n",
    "value = torch.randn(32, 20, 64)  # Example value tensor\n",
    "inputs = \"\"\"\n",
    "query = torch.randn(32, 10, 64)  # Example query tensor\n",
    "key = torch.randn(32, 20, 64)  # Example key tensor\n",
    "value = torch.randn(32, 20, 64)  # Example value tensor\n",
    "inputs = (query, key, value)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allows the model to jointly attend to information\n",
      "    from different representation subspaces as described in the paper:\n",
      "    `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\n",
      "\n",
      "    Multi-Head Attention is defined as:\n",
      "\n",
      "    .. math::\n",
      "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
      "\n",
      "    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
      "\n",
      "    ``nn.MultiHeadAttention`` will use the optimized implementations of\n",
      "    ``scaled_dot_product_attention()`` when possible.\n",
      "\n",
      "    In addition to support for the new ``scaled_dot_product_attention()``\n",
      "    function, for speeding up Inference, MHA will use\n",
      "    fastpath inference with support for Nested Tensors, iff:\n",
      "\n",
      "    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).\n",
      "    - inputs are batched (3D) with ``batch_first==True``\n",
      "    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n",
      "    - training is disabled (using ``.eval()``)\n",
      "    - ``add_bias_kv`` is ``False``\n",
      "    - ``add_zero_attn`` is ``False``\n",
      "    - ``batch_first`` is ``True`` and the input is batched\n",
      "    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\n",
      "    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n",
      "      nor ``attn_mask`` is passed\n",
      "    - autocast is disabled\n",
      "\n",
      "    If the optimized inference fastpath implementation is in use, a\n",
      "    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n",
      "    ``query``/``key``/``value`` to represent padding more efficiently than using a\n",
      "    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n",
      "    will be returned, and an additional speedup proportional to the fraction of the input\n",
      "    that is padding can be expected.\n",
      "\n",
      "    Args:\n",
      "        embed_dim: Total dimension of the model.\n",
      "        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n",
      "            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n",
      "        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n",
      "        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n",
      "        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n",
      "        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n",
      "            Default: ``False``.\n",
      "        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n",
      "        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n",
      "        batch_first: If ``True``, then the input and output tensors are provided\n",
      "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> # xdoctest: +SKIP\n",
      "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
      "\n",
      "    .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n",
      "         https://arxiv.org/abs/2205.14135\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(class_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.randn(32, 10, 64)  # Example query tensor\n",
    "key = torch.randn(32, 20, 64)  # Example key tensor\n",
    "value = torch.randn(32, 20, 64)  # Example value tensor\n",
    "inputs = (query, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch_model = nn.modules.activation.MultiheadAttention(\n",
    "    embed_dim=64,\n",
    "    num_heads=8,\n",
    "    dropout=0.1,\n",
    "    bias=True,\n",
    "    add_bias_kv=False,\n",
    "    add_zero_attn=False,\n",
    "    kdim=None,\n",
    "    vdim=None,\n",
    "    batch_first=False\n",
    ")\n",
    "\n",
    "query = torch.randn(32, 10, 64)  # Example query tensor\n",
    "key = torch.randn(32, 20, 64)  # Example key tensor\n",
    "value = torch.randn(32, 20, 64)  # Example value tensor\n",
    "output, attn_output_weights = torch_model(query, key, value)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 80, 8]' is invalid for input of size 40960",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[212], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m key \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m64\u001b[39m)  \u001b[38;5;66;03m# Example key tensor\u001b[39;00m\n\u001b[1;32m     15\u001b[0m value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m64\u001b[39m)  \u001b[38;5;66;03m# Example value tensor\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:5346\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5344\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(tgt_len, bsz \u001b[38;5;241m*\u001b[39m num_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 5346\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5348\u001b[0m     \u001b[38;5;66;03m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001b[39;00m\n\u001b[1;32m   5349\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m static_k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m bsz \u001b[38;5;241m*\u001b[39m num_heads, \\\n\u001b[1;32m   5350\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting static_k.size(0) of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbsz\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mnum_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatic_k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 80, 8]' is invalid for input of size 40960"
     ]
    }
   ],
   "source": [
    "\n",
    "torch_model = nn.modules.activation.MultiheadAttention(\n",
    "    embed_dim=64,\n",
    "    num_heads=8,\n",
    "    dropout=0.1,\n",
    "    bias=True,\n",
    "    add_bias_kv=False,\n",
    "    add_zero_attn=False,\n",
    "    kdim=None,\n",
    "    vdim=None,\n",
    "    batch_first=False\n",
    ")\n",
    "\n",
    "query = torch.randn(32, 10, 64)  # Example query tensor\n",
    "key = torch.randn(32, 20, 64)  # Example key tensor\n",
    "value = torch.randn(32, 20, 64)  # Example value tensor\n",
    "output, attn_output_weights = torch_model(query, key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "<<Executed Code>>\n",
    "\n",
    "\n",
    "torch_model = nn.modules.activation.MultiheadAttention(\n",
    "    embed_dim=64,\n",
    "    num_heads=8,\n",
    "    dropout=0.1,\n",
    "    bias=True,\n",
    "    add_bias_kv=False,\n",
    "    add_zero_attn=False,\n",
    "    kdim=None,\n",
    "    vdim=None,\n",
    "    batch_first=False\n",
    ")\n",
    "\n",
    "query = torch.randn(32, 10, 64)  # Example query tensor\n",
    "key = torch.randn(32, 20, 64)  # Example key tensor\n",
    "value = torch.randn(32, 20, 64)  # Example value tensor\n",
    "output, attn_output_weights = torch_model(query, key, value)\n",
    "\n",
    "\n",
    "<<Error>>\n",
    "\n",
    "File ~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241, in MultiheadAttention.forward(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\n",
    "   1227     attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "   1228         query, key, value, self.embed_dim, self.num_heads,\n",
    "   1229         self.in_proj_weight, self.in_proj_bias,\n",
    "   (...)\n",
    "   1238         average_attn_weights=average_attn_weights,\n",
    "   1239         is_causal=is_causal)\n",
    "   1240 else:\n",
    "-> 1241     attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "   1242         query, key, value, self.embed_dim, self.num_heads,\n",
    "   1243         self.in_proj_weight, self.in_proj_bias,\n",
    "   1244         self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "   1245         self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "   1246         training=self.training,\n",
    "   1247         key_padding_mask=key_padding_mask,\n",
    "   1248         need_weights=need_weights,\n",
    "   1249         attn_mask=attn_mask,\n",
    "   1250         average_attn_weights=average_attn_weights,\n",
    "   1251         is_causal=is_causal)\n",
    "   1252 if self.batch_first and is_batched:\n",
    "   1253     return attn_output.transpose(1, 0), attn_output_weights\n",
    "\n",
    "File ~/.local/lib/python3.10/site-packages/torch/nn/functional.py:5346, in multi_head_attention_forward(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\n",
    "   5344 q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "   5345 if static_k is None:\n",
    "-> 5346     k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "   5347 else:\n",
    "   5348     # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
    "   5349     assert static_k.size(0) == bsz * num_heads, \\\n",
    "   5350         f\"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}\"\n",
    "\n",
    "RuntimeError: shape '[32, 80, 8]' is invalid for input of size 40960\n",
    "\n",
    "<<Task>>\n",
    "\n",
    "Given the error, rewrite the code in python code block.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 80, 8]' is invalid for input of size 40960",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[217], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m key \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m64\u001b[39m)  \u001b[38;5;66;03m# Example key tensor\u001b[39;00m\n\u001b[1;32m     15\u001b[0m value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m64\u001b[39m)  \u001b[38;5;66;03m# Example value tensor\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:5346\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5344\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(tgt_len, bsz \u001b[38;5;241m*\u001b[39m num_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 5346\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5348\u001b[0m     \u001b[38;5;66;03m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001b[39;00m\n\u001b[1;32m   5349\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m static_k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m bsz \u001b[38;5;241m*\u001b[39m num_heads, \\\n\u001b[1;32m   5350\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting static_k.size(0) of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbsz\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mnum_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatic_k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 80, 8]' is invalid for input of size 40960"
     ]
    }
   ],
   "source": [
    "torch_model = nn.modules.activation.MultiheadAttention(\n",
    "    embed_dim=64,\n",
    "    num_heads=8,\n",
    "    dropout=0.1,\n",
    "    bias=True,\n",
    "    add_bias_kv=False,\n",
    "    add_zero_attn=False,\n",
    "    kdim=None,\n",
    "    vdim=None,\n",
    "    batch_first=False\n",
    ")\n",
    "\n",
    "query = torch.randn(32, 10, 64)  # Example query tensor\n",
    "key = torch.randn(32, 20, 64)  # Example key tensor\n",
    "value = torch.randn(32, 20, 64)  # Example value tensor\n",
    "output, attn_output_weights = torch_model(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch_model = nn.modules.activation.MultiheadAttention(\n",
    "    embed_dim=64,\n",
    "    num_heads=8,\n",
    "    dropout=0.1,\n",
    "    bias=True,\n",
    "    add_bias_kv=False,\n",
    "    add_zero_attn=False,\n",
    "    kdim=None,\n",
    "    vdim=None,\n",
    "    batch_first=True  # Change batch_first to True\n",
    ")\n",
    "\n",
    "query = torch.randn(32, 10, 64)  # Example query tensor\n",
    "key = torch.randn(32, 20, 64)  # Example key tensor\n",
    "value = torch.randn(32, 20, 64)  # Example value tensor\n",
    "output, attn_output_weights = torch_model(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def test_MultiheadAttention_converter(self):\n",
      "    # Initialize the model directly from its constructor\n",
      "    \n",
      "torch_model = nn.modules.activation.MultiheadAttention(\n",
      "    embed_dim=256,\n",
      "    num_heads=8,\n",
      "    dropout=0.1,\n",
      "    bias=True,\n",
      "    add_bias_kv=False,\n",
      "    add_zero_attn=False,\n",
      "    kdim=None,\n",
      "    vdim=None,\n",
      "    batch_first=False\n",
      ")\n",
      "\n",
      "    torch_model.eval()\n",
      "\t# Initialize the model and input tensor\n",
      "    \n",
      "query = torch.randn(32, 10, 64)  # Example query tensor\n",
      "key = torch.randn(32, 20, 64)  # Example key tensor\n",
      "value = torch.randn(32, 20, 64)  # Example value tensor\n",
      "inputs = (query, key, value)\n",
      "\n",
      "\n",
      "    # Convert the model and ensure the HTML trace is saved\n",
      "    keras_model = nobuco.pytorch_to_keras(\n",
      "        torch_model,\n",
      "        args=[*inputs], kwargs=None,\n",
      "        inputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
      "        outputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
      "        save_trace_html=True\n",
      "    )\n",
      "\n",
      "    # Read the contents of the trace.html file\n",
      "    with open('trace.html', 'r', encoding='utf-8') as file:\n",
      "        trace_html = file.read()\n",
      "\n",
      "    # Assertions for the content of trace_html\n",
      "    self.assertNotIn('Max diff', trace_html, \"The trace HTML should not contain 'Max diff'\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_unittest_template(\"MultiheadAttention\", phase1_output, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4 32K fhgenie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "multihead_attn = nn.MultiheadAttention(\n",
    "    embed_dim=512, \n",
    "    num_heads=8\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = nn.modules.activation.MultiheadAttention(\n",
    "    embed_dim=512, \n",
    "    num_heads=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1_output = '''\n",
    "multihead_attn = nn.MultiheadAttention(\n",
    "    embed_dim=512, \n",
    "    num_heads=8\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<<Instruction>>\n",
      "\n",
      "Given the context, complete the <<Task>>.\n",
      "\n",
      "<<Documentation for nn.modules.activation.MultiheadAttention>>\n",
      "\n",
      "Allows the model to jointly attend to information\n",
      "    from different representation subspaces as described in the paper:\n",
      "    `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\n",
      "\n",
      "    Multi-Head Attention is defined as:\n",
      "\n",
      "    .. math::\n",
      "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
      "\n",
      "    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
      "\n",
      "    ``nn.MultiHeadAttention`` will use the optimized implementations of\n",
      "    ``scaled_dot_product_attention()`` when possible.\n",
      "\n",
      "    In addition to support for the new ``scaled_dot_product_attention()``\n",
      "    function, for speeding up Inference, MHA will use\n",
      "    fastpath inference with support for Nested Tensors, iff:\n",
      "\n",
      "    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).\n",
      "    - inputs are batched (3D) with ``batch_first==True``\n",
      "    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n",
      "    - training is disabled (using ``.eval()``)\n",
      "    - ``add_bias_kv`` is ``False``\n",
      "    - ``add_zero_attn`` is ``False``\n",
      "    - ``batch_first`` is ``True`` and the input is batched\n",
      "    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\n",
      "    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n",
      "      nor ``attn_mask`` is passed\n",
      "    - autocast is disabled\n",
      "\n",
      "    If the optimized inference fastpath implementation is in use, a\n",
      "    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n",
      "    ``query``/``key``/``value`` to represent padding more efficiently than using a\n",
      "    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n",
      "    will be returned, and an additional speedup proportional to the fraction of the input\n",
      "    that is padding can be expected.\n",
      "\n",
      "    Args:\n",
      "        embed_dim: Total dimension of the model.\n",
      "        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n",
      "            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n",
      "        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n",
      "        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n",
      "        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n",
      "        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n",
      "            Default: ``False``.\n",
      "        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n",
      "        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n",
      "        batch_first: If ``True``, then the input and output tensors are provided\n",
      "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> # xdoctest: +SKIP\n",
      "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
      "\n",
      "    .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n",
      "         https://arxiv.org/abs/2205.14135\n",
      "\n",
      "    \n",
      "\n",
      "<<Example>>\n",
      "\n",
      "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
      "\n",
      "target: \n",
      "torch_model = torch.nn.Conv2d(\n",
      "    in_channels=10, \n",
      "    out_channels=20, \n",
      "    kernel_size=3\n",
      ")\n",
      "\n",
      "output:\n",
      "\n",
      "```python\n",
      "inputs = [torch.randn(1, 10, 32, 32)]\n",
      "output = torch_model.forward(*inputs)\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "<<Task>>\n",
      "\n",
      "Write the inputs of the target's forward function, and the line of forward as well in python code block.\n",
      "\n",
      "target: \n",
      "multihead_attn = nn.MultiheadAttention(\n",
      "    embed_dim=512, \n",
      "    num_heads=8\n",
      ")\n",
      "\n",
      "\n",
      "output:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_phase2_prompt(target, class_doc, phase2_example, phase1_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.randn(5, 32, 512)\n",
    "key = torch.randn(5, 32, 512)\n",
    "value = torch.randn(5, 32, 512)\n",
    "output, weights = multihead_attn.forward(query, key, value)\n",
    "inputs_tensor = (query, key, value)\n",
    "inputs = \"\"\"\n",
    "query = torch.randn(5, 32, 512)\n",
    "key = torch.randn(5, 32, 512)\n",
    "value = torch.randn(5, 32, 512)\n",
    "inputs = [query, key, value]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_tensor = (query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def test_MultiheadAttention_converter(self):\n",
      "    # Initialize the model directly from its constructor\n",
      "    \n",
      "multihead_attn = nn.MultiheadAttention(\n",
      "    embed_dim=512, \n",
      "    num_heads=8\n",
      ")\n",
      "\n",
      "    torch_model.eval()\n",
      "\t# Initialize the model and input tensor\n",
      "    \n",
      "query = torch.randn(5, 32, 512)\n",
      "key = torch.randn(5, 32, 512)\n",
      "value = torch.randn(5, 32, 512)\n",
      "inputs = (query, key, value)\n",
      "\n",
      "\n",
      "\n",
      "    # Convert the model and ensure the HTML trace is saved\n",
      "    keras_model = nobuco.pytorch_to_keras(\n",
      "        torch_model,\n",
      "        args=[*inputs], kwargs=None,\n",
      "        inputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
      "        outputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
      "        save_trace_html=True\n",
      "    )\n",
      "\n",
      "    # Read the contents of the trace.html file\n",
      "    with open('trace.html', 'r', encoding='utf-8') as file:\n",
      "        trace_html = file.read()\n",
      "\n",
      "    # Assertions for the content of trace_html\n",
      "    self.assertNotIn('Max diff', trace_html, \"The trace HTML should not contain 'Max diff'\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_unittest_template(\"MultiheadAttention\", phase1_output, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.activation.MultiheadAttention"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(multihead_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/nobuco/nobuco/converters/validation.py:66: UserWarning: Validation exception on node 'MultiheadAttention': Tensor shapes of output #1 don't match: (Pytorch) [5, 32, 32] vs [5, 8, 32, 32] (Tensorflow)\n",
      "  warnings.warn(f\"Validation exception on node '{op_type.__name__}': {e}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/nobuco/nobuco/converters/validation.py\", line 47, in validate\n",
      "    diffs = validate_diff_default(keras_op, pytorch_op, input_args, input_kwargs, output_tensors)\n",
      "  File \"/workspaces/nobuco/nobuco/converters/validation.py\", line 89, in validate_diff_default\n",
      "    raise Exception(f\"Tensor shapes of output #{i} don't match: (Pytorch) {list(t_pt.shape)} vs {list(t_tf.shape)} (Tensorflow)\")\n",
      "Exception: Tensor shapes of output #1 don't match: (Pytorch) [5, 32, 32] vs [5, 8, 32, 32] (Tensorflow)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legend:\n",
      "    \u001b[32mGreen\u001b[0m — conversion successful\n",
      "    \u001b[33mYellow\u001b[0m — conversion imprecise\n",
      "    \u001b[31mRed\u001b[0m — conversion failed\n",
      "    \u001b[31m\u001b[7mRed\u001b[0m — no converter found\n",
      "    \u001b[0m\u001b[1mBold\u001b[0m — conversion applied directly\n",
      "    * — subgraph reused\n",
      "    \u001b[7mTensor\u001b[0m — this output is not dependent on any of subgraph's input tensors\n",
      "    \u001b[4mTensor\u001b[0m — this input is a parameter / constant\n",
      "    \u001b[90mTensor\u001b[0m — this tensor is useless\n",
      "\n",
      "\u001b[90m I \u001b[0m\u001b[90m File \"/workspaces/nobuco/nobuco/trace/trace.py\", line 460\u001b[0m \n",
      "\u001b[90m D \u001b[0m\u001b[90m File \"/home/codespace/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py\", line 906 \u001b[0m \n",
      "\u001b[31m\u001b[1m C \u001b[0m\u001b[90m File \"/workspaces/nobuco/nobuco/node_converters/attention.py\", line 14 \u001b[0m \n",
      "\u001b[31m\u001b[1mMultiheadAttention[torch.nn.modules.activation]\u001b[0m(float32_0<5,32,512>\u001b[0m, float32_1<5,32,512>\u001b[0m, float32_2<5,32,512>\u001b[0m) -> (float32_37<5,32,512>\u001b[0m, float32_36<5,32,32>\u001b[0m)\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0mtranspose[torch.Tensor]\u001b[0m(float32_0<5,32,512>\u001b[0m, 1, 0) -> float32_3<32,5,512>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0mtranspose[torch.Tensor]\u001b[0m(float32_1<5,32,512>\u001b[0m, 1, 0) -> float32_4<32,5,512>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0mtranspose[torch.Tensor]\u001b[0m(float32_2<5,32,512>\u001b[0m, 1, 0) -> float32_5<32,5,512>\u001b[0m\n",
      "\u001b[31m\u001b[1m ├·\u001b[0m \u001b[0mmulti_head_attention_forward[torch.nn.functional]\u001b[0m(float32_3<32,5,512>\u001b[0m, float32_4<32,5,512>\u001b[0m, float32_5<32,5,512>\u001b[0m, 512, 8, float32_6<1536,512>\u001b[0m, float32_7<1536>\u001b[0m, None, None, False, 0.0, float32_8<512,512>\u001b[0m, float32_9<512>\u001b[0m, training=True, key_padding_mask=None, need_weights=True, attn_mask=None, average_attn_weights=True, is_causal=False) -> (float32_34<32,5,512>\u001b[0m, float32_36<5,32,32>\u001b[0m)\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0m_in_projection_packed[torch.nn.functional]\u001b[0m(float32_3<32,5,512>\u001b[0m, float32_4<32,5,512>\u001b[0m, float32_5<32,5,512>\u001b[0m, float32_6<1536,512>\u001b[0m, float32_7<1536>\u001b[0m) -> (float32_16<32,5,512>\u001b[0m, float32_17<32,5,512>\u001b[0m, float32_18<32,5,512>\u001b[0m)\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mchunk[torch.Tensor]\u001b[0m(float32_6<1536,512>\u001b[0m, 3) -> (float32_10<512,512>\u001b[0m, float32_11<512,512>\u001b[0m, float32_12<512,512>\u001b[0m)\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mchunk[torch.Tensor]\u001b[0m(float32_7<1536>\u001b[0m, 3) -> (float32_13<512>\u001b[0m, float32_14<512>\u001b[0m, float32_15<512>\u001b[0m)\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0m ├·\u001b[0m \u001b[0mlinear[torch.nn.functional]\u001b[0m(float32_3<32,5,512>\u001b[0m, float32_10<512,512>\u001b[0m, float32_13<512>\u001b[0m) -> float32_16<32,5,512>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0m ├·\u001b[0m \u001b[0mlinear[torch.nn.functional]\u001b[0m(float32_4<32,5,512>\u001b[0m, float32_11<512,512>\u001b[0m, float32_14<512>\u001b[0m) -> float32_17<32,5,512>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0m └·\u001b[0m \u001b[0mlinear[torch.nn.functional]\u001b[0m(float32_5<32,5,512>\u001b[0m, float32_12<512,512>\u001b[0m, float32_15<512>\u001b[0m) -> float32_18<32,5,512>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mview[torch.Tensor]\u001b[0m(float32_16<32,5,512>\u001b[0m, 32, 40, 64) -> float32_19<32,40,64>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mtranspose[torch.Tensor]\u001b[0m(float32_19<32,40,64>\u001b[0m, 0, 1) -> float32_20<40,32,64>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mview[torch.Tensor]\u001b[0m(float32_17<32,5,512>\u001b[0m, 32, 40, 64) -> float32_21<32,40,64>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mtranspose[torch.Tensor]\u001b[0m(float32_21<32,40,64>\u001b[0m, 0, 1) -> float32_22<40,32,64>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mview[torch.Tensor]\u001b[0m(float32_18<32,5,512>\u001b[0m, 32, 40, 64) -> float32_23<32,40,64>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mtranspose[torch.Tensor]\u001b[0m(float32_23<32,40,64>\u001b[0m, 0, 1) -> float32_24<40,32,64>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0m__truediv__[torch.Tensor]\u001b[0m(float32_20<40,32,64>\u001b[0m, 8.0) -> float32_25<40,32,64>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mtranspose[torch.Tensor]\u001b[0m(float32_22<40,32,64>\u001b[0m, -2, -1) -> float32_26<40,64,32>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mbmm[torch]\u001b[0m(float32_25<40,32,64>\u001b[0m, float32_26<40,64,32>\u001b[0m) -> float32_27<40,32,32>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0msoftmax[torch.nn.functional]\u001b[0m(float32_27<40,32,32>\u001b[0m, dim=-1) -> float32_28<40,32,32>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0m └·\u001b[0m \u001b[0msoftmax[torch.Tensor]\u001b[0m(float32_27<40,32,32>\u001b[0m, -1) -> float32_28<40,32,32>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mbmm[torch]\u001b[0m(float32_28<40,32,32>\u001b[0m, float32_24<40,32,64>\u001b[0m) -> float32_29<40,32,64>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mtranspose[torch.Tensor]\u001b[0m(float32_29<40,32,64>\u001b[0m, 0, 1) -> float32_30<32,40,64>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mcontiguous[torch.Tensor]\u001b[0m(float32_30<32,40,64>\u001b[0m) -> float32_31<32,40,64>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mview[torch.Tensor]\u001b[0m(float32_31<32,40,64>\u001b[0m, 160, 512) -> float32_32<160,512>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mlinear[torch.nn.functional]\u001b[0m(float32_32<160,512>\u001b[0m, float32_8<512,512>\u001b[0m, float32_9<512>\u001b[0m) -> float32_33<160,512>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m ├·\u001b[0m \u001b[0mview[torch.Tensor]\u001b[0m(float32_33<160,512>\u001b[0m, 32, 5, 512) -> float32_34<32,5,512>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m │ \u001b[0m \u001b[0mview[torch.Tensor]\u001b[0m(float32_28<40,32,32>\u001b[0m, 5, 8, 32, 32) -> float32_35<5,8,32,32>\u001b[0m\n",
      "\u001b[31m\u001b[1m │ \u001b[0m \u001b[0m └·\u001b[0m \u001b[0mmean[torch.Tensor]\u001b[0m(float32_35<5,8,32,32>\u001b[0m, dim=1) -> float32_36<5,32,32>\u001b[0m\n",
      "\u001b[31m\u001b[1m └·\u001b[0m \u001b[0mtranspose[torch.Tensor]\u001b[0m(float32_34<32,5,512>\u001b[0m, 1, 0) -> float32_37<5,32,512>\u001b[0m\n",
      "\n",
      "Conversion complete. Elapsed time: 0.38 sec.\n"
     ]
    }
   ],
   "source": [
    "import nobuco \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "torch_model = nn.MultiheadAttention(\n",
    "    embed_dim=512, \n",
    "    num_heads=8,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "query = torch.randn(1, 32, 512)\n",
    "key = torch.randn(1, 32, 512)\n",
    "value = torch.randn(1, 32, 512)\n",
    "output, weights = torch_model.forward(query, key, value)\n",
    "inputs = (query, key, value)\n",
    "\n",
    "keras_model = nobuco.pytorch_to_keras(\n",
    "    torch_model,\n",
    "    args=[*inputs_tensor], kwargs=None,\n",
    "    inputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
    "    outputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
