{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest_automation import get_preprocessed_itemholder\n",
    "\n",
    "itemholder = get_preprocessed_itemholder(\"math.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(name='torch.sin', docs='\\nsin(input, *, out=None) -> Tensor\\n\\nReturns a new tensor with the sine of the elements of :attr:`input`.\\n\\n.. math::\\n    \\\\text{out}_{i} = \\\\sin(\\\\text{input}_{i})\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.randn(4)\\n    >>> a\\n    tensor([-0.5461,  0.1347, -2.7266, -0.2746])\\n    >>> torch.sin(a)\\n    tensor([-0.5194,  0.1343, -0.4032, -0.2711])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.randn(4)\\n>>> a\\ntensor([-0.5461,  0.1347, -2.7266, -0.2746])\\n>>> torch.sin(a)\\ntensor([-0.5194,  0.1343, -0.4032, -0.2711])\\n'}], meta_data={}),\n",
       " Item(name='torch.cos', docs='\\ncos(input, *, out=None) -> Tensor\\n\\nReturns a new tensor with the cosine  of the elements of :attr:`input`.\\n\\n.. math::\\n    \\\\text{out}_{i} = \\\\cos(\\\\text{input}_{i})\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.randn(4)\\n    >>> a\\n    tensor([ 1.4309,  1.2706, -0.8562,  0.9796])\\n    >>> torch.cos(a)\\n    tensor([ 0.1395,  0.2957,  0.6553,  0.5574])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.randn(4)\\n>>> a\\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\\n>>> torch.cos(a)\\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])\\n'}], meta_data={}),\n",
       " Item(name='torch.add', docs='\\nadd(input, other, *, alpha=1, out=None) -> Tensor\\n\\nAdds :attr:`other`, scaled by :attr:`alpha`, to :attr:`input`.\\n\\n.. math::\\n    \\\\text{{out}}_i = \\\\text{{input}}_i + \\\\text{{alpha}} \\\\times \\\\text{{other}}_i\\n\\n\\nSupports :ref:`broadcasting to a common shape <broadcasting-semantics>`,\\n:ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n    other (Tensor or Number): the tensor or number to add to :attr:`input`.\\n\\nKeyword arguments:\\n    alpha (Number): the multiplier for :attr:`other`.\\n    out (Tensor, optional): the output tensor.\\n\\nExamples::\\n\\n    >>> a = torch.randn(4)\\n    >>> a\\n    tensor([ 0.0202,  1.0985,  1.3506, -0.6056])\\n    >>> torch.add(a, 20)\\n    tensor([ 20.0202,  21.0985,  21.3506,  19.3944])\\n\\n    >>> b = torch.randn(4)\\n    >>> b\\n    tensor([-0.9732, -0.3497,  0.6245,  0.4022])\\n    >>> c = torch.randn(4, 1)\\n    >>> c\\n    tensor([[ 0.3743],\\n            [-1.7724],\\n            [-0.5811],\\n            [-0.8017]])\\n    >>> torch.add(b, c, alpha=10)\\n    tensor([[  2.7695,   3.3930,   4.3672,   4.1450],\\n            [-18.6971, -18.0736, -17.0994, -17.3216],\\n            [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\\n            [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\nother (Tensor or Number): the tensor or number to add to :attr:`input`.\\n'}, {'header': 'Keyword arguments', 'content': 'alpha (Number): the multiplier for :attr:`other`.\\nout (Tensor, optional): the output tensor.\\n'}, {'header': 'Examples', 'content': '\\n>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.0202,  1.0985,  1.3506, -0.6056])\\n>>> torch.add(a, 20)\\ntensor([ 20.0202,  21.0985,  21.3506,  19.3944])\\n\\n>>> b = torch.randn(4)\\n>>> b\\ntensor([-0.9732, -0.3497,  0.6245,  0.4022])\\n>>> c = torch.randn(4, 1)\\n>>> c\\ntensor([[ 0.3743],\\n        [-1.7724],\\n        [-0.5811],\\n        [-0.8017]])\\n>>> torch.add(b, c, alpha=10)\\ntensor([[  2.7695,   3.3930,   4.3672,   4.1450],\\n        [-18.6971, -18.0736, -17.0994, -17.3216],\\n        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\\n        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])\\n'}], meta_data={}),\n",
       " Item(name='torch.sub', docs='\\nsub(input, other, *, alpha=1, out=None) -> Tensor\\n\\nSubtracts :attr:`other`, scaled by :attr:`alpha`, from :attr:`input`.\\n\\n.. math::\\n    \\\\text{{out}}_i = \\\\text{{input}}_i - \\\\text{{alpha}} \\\\times \\\\text{{other}}_i\\n\\n\\nSupports :ref:`broadcasting to a common shape <broadcasting-semantics>`,\\n:ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n    other (Tensor or Number): the tensor or number to subtract from :attr:`input`.\\n\\nKeyword args:\\n    alpha (Number): the multiplier for :attr:`other`.\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.tensor((1, 2))\\n    >>> b = torch.tensor((0, 1))\\n    >>> torch.sub(a, b, alpha=2)\\n    tensor([1, 0])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\nother (Tensor or Number): the tensor or number to subtract from :attr:`input`.\\n'}, {'header': 'Keyword args', 'content': 'alpha (Number): the multiplier for :attr:`other`.\\nout (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.tensor((1, 2))\\n>>> b = torch.tensor((0, 1))\\n>>> torch.sub(a, b, alpha=2)\\ntensor([1, 0])\\n'}], meta_data={}),\n",
       " Item(name='torch.Tensor.neg', docs='\\nneg() -> Tensor\\n\\nSee :func:`torch.neg`\\n', parsed_docs=[], meta_data={}),\n",
       " Item(name='torch.mul', docs='\\nmul(input, other, *, out=None) -> Tensor\\n\\nMultiplies :attr:`input` by :attr:`other`.\\n\\n\\n.. math::\\n    \\\\text{out}_i = \\\\text{input}_i \\\\times \\\\text{other}_i\\n\\n\\nSupports :ref:`broadcasting to a common shape <broadcasting-semantics>`,\\n:ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n    other (Tensor or Number) - the tensor or number to multiply input by.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExamples::\\n\\n    >>> a = torch.randn(3)\\n    >>> a\\n    tensor([ 0.2015, -0.4255,  2.6087])\\n    >>> torch.mul(a, 100)\\n    tensor([  20.1494,  -42.5491,  260.8663])\\n\\n    >>> b = torch.randn(4, 1)\\n    >>> b\\n    tensor([[ 1.1207],\\n            [-0.3137],\\n            [ 0.0700],\\n            [ 0.8378]])\\n    >>> c = torch.randn(1, 4)\\n    >>> c\\n    tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])\\n    >>> torch.mul(b, c)\\n    tensor([[ 0.5767,  0.1363, -0.5877,  2.5083],\\n            [-0.1614, -0.0382,  0.1645, -0.7021],\\n            [ 0.0360,  0.0085, -0.0367,  0.1567],\\n            [ 0.4312,  0.1019, -0.4394,  1.8753]])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\nother (Tensor or Number) - the tensor or number to multiply input by.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Examples', 'content': '\\n>>> a = torch.randn(3)\\n>>> a\\ntensor([ 0.2015, -0.4255,  2.6087])\\n>>> torch.mul(a, 100)\\ntensor([  20.1494,  -42.5491,  260.8663])\\n\\n>>> b = torch.randn(4, 1)\\n>>> b\\ntensor([[ 1.1207],\\n        [-0.3137],\\n        [ 0.0700],\\n        [ 0.8378]])\\n>>> c = torch.randn(1, 4)\\n>>> c\\ntensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])\\n>>> torch.mul(b, c)\\ntensor([[ 0.5767,  0.1363, -0.5877,  2.5083],\\n        [-0.1614, -0.0382,  0.1645, -0.7021],\\n        [ 0.0360,  0.0085, -0.0367,  0.1567],\\n        [ 0.4312,  0.1019, -0.4394,  1.8753]])\\n'}], meta_data={}),\n",
       " Item(name='torch.Tensor.mul', docs='\\nmul(value) -> Tensor\\n\\nSee :func:`torch.mul`.\\n', parsed_docs=[], meta_data={}),\n",
       " Item(name='torch.Tensor.div', docs='\\ndiv(value, *, rounding_mode=None) -> Tensor\\n\\nSee :func:`torch.div`\\n', parsed_docs=[], meta_data={}),\n",
       " Item(name='torch.floor_divide', docs=\"\\nfloor_divide(input, other, *, out=None) -> Tensor\\n\\n.. note::\\n\\n    Before PyTorch 1.13 :func:`torch.floor_divide` incorrectly performed\\n    truncation division. To restore the previous behavior use\\n    :func:`torch.div` with ``rounding_mode='trunc'``.\\n\\nComputes :attr:`input` divided by :attr:`other`, elementwise, and floors\\nthe result.\\n\\n.. math::\\n    \\\\text{{out}}_i = \\\\text{floor} \\\\left( \\\\frac{{\\\\text{{input}}_i}}{{\\\\text{{other}}_i}} \\\\right)\\n\\n\\n\\nSupports broadcasting to a common shape, type promotion, and integer and float inputs.\\n\\nArgs:\\n    input (Tensor or Number): the dividend\\n    other (Tensor or Number): the divisor\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.tensor([4.0, 3.0])\\n    >>> b = torch.tensor([2.0, 2.0])\\n    >>> torch.floor_divide(a, b)\\n    tensor([2.0, 1.0])\\n    >>> torch.floor_divide(a, 1.4)\\n    tensor([2.0, 2.0])\\n\", parsed_docs=[{'header': 'Args', 'content': 'input (Tensor or Number): the dividend\\nother (Tensor or Number): the divisor\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.tensor([4.0, 3.0])\\n>>> b = torch.tensor([2.0, 2.0])\\n>>> torch.floor_divide(a, b)\\ntensor([2.0, 1.0])\\n>>> torch.floor_divide(a, 1.4)\\ntensor([2.0, 2.0])\\n'}], meta_data={}),\n",
       " Item(name='torch.sqrt', docs='\\nsqrt(input, *, out=None) -> Tensor\\n\\nReturns a new tensor with the square-root of the elements of :attr:`input`.\\n\\n.. math::\\n    \\\\text{out}_{i} = \\\\sqrt{\\\\text{input}_{i}}\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.randn(4)\\n    >>> a\\n    tensor([-2.0755,  1.0226,  0.0831,  0.4806])\\n    >>> torch.sqrt(a)\\n    tensor([    nan,  1.0112,  0.2883,  0.6933])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.randn(4)\\n>>> a\\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\\n>>> torch.sqrt(a)\\ntensor([    nan,  1.0112,  0.2883,  0.6933])\\n'}], meta_data={}),\n",
       " Item(name='torch.rsqrt', docs='\\nrsqrt(input, *, out=None) -> Tensor\\n\\nReturns a new tensor with the reciprocal of the square-root of each of\\nthe elements of :attr:`input`.\\n\\n.. math::\\n    \\\\text{out}_{i} = \\\\frac{1}{\\\\sqrt{\\\\text{input}_{i}}}\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.randn(4)\\n    >>> a\\n    tensor([-0.0370,  0.2970,  1.5420, -0.9105])\\n    >>> torch.rsqrt(a)\\n    tensor([    nan,  1.8351,  0.8053,     nan])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.randn(4)\\n>>> a\\ntensor([-0.0370,  0.2970,  1.5420, -0.9105])\\n>>> torch.rsqrt(a)\\ntensor([    nan,  1.8351,  0.8053,     nan])\\n'}], meta_data={}),\n",
       " Item(name='torch.square', docs='\\nsquare(input, *, out=None) -> Tensor\\n\\nReturns a new tensor with the square of the elements of :attr:`input`.\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.randn(4)\\n    >>> a\\n    tensor([-2.0755,  1.0226,  0.0831,  0.4806])\\n    >>> torch.square(a)\\n    tensor([ 4.3077,  1.0457,  0.0069,  0.2310])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.randn(4)\\n>>> a\\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\\n>>> torch.square(a)\\ntensor([ 4.3077,  1.0457,  0.0069,  0.2310])\\n'}], meta_data={}),\n",
       " Item(name='torch.pow', docs='\\npow(input, exponent, *, out=None) -> Tensor\\n\\nTakes the power of each element in :attr:`input` with :attr:`exponent` and\\nreturns a tensor with the result.\\n\\n:attr:`exponent` can be either a single ``float`` number or a `Tensor`\\nwith the same number of elements as :attr:`input`.\\n\\nWhen :attr:`exponent` is a scalar value, the operation applied is:\\n\\n.. math::\\n    \\\\text{out}_i = x_i ^ \\\\text{exponent}\\n\\nWhen :attr:`exponent` is a tensor, the operation applied is:\\n\\n.. math::\\n    \\\\text{out}_i = x_i ^ {\\\\text{exponent}_i}\\n\\nWhen :attr:`exponent` is a tensor, the shapes of :attr:`input`\\nand :attr:`exponent` must be :ref:`broadcastable <broadcasting-semantics>`.\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n    exponent (float or tensor): the exponent value\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.randn(4)\\n    >>> a\\n    tensor([ 0.4331,  1.2475,  0.6834, -0.2791])\\n    >>> torch.pow(a, 2)\\n    tensor([ 0.1875,  1.5561,  0.4670,  0.0779])\\n    >>> exp = torch.arange(1., 5.)\\n\\n    >>> a = torch.arange(1., 5.)\\n    >>> a\\n    tensor([ 1.,  2.,  3.,  4.])\\n    >>> exp\\n    tensor([ 1.,  2.,  3.,  4.])\\n    >>> torch.pow(a, exp)\\n    tensor([   1.,    4.,   27.,  256.])\\n\\n.. function:: pow(self, exponent, *, out=None) -> Tensor\\n   :noindex:\\n\\n:attr:`self` is a scalar ``float`` value, and :attr:`exponent` is a tensor.\\nThe returned tensor :attr:`out` is of the same shape as :attr:`exponent`\\n\\nThe operation applied is:\\n\\n.. math::\\n    \\\\text{out}_i = \\\\text{self} ^ {\\\\text{exponent}_i}\\n\\nArgs:\\n    self (float): the scalar base value for the power operation\\n    exponent (Tensor): the exponent tensor\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> exp = torch.arange(1., 5.)\\n    >>> base = 2\\n    >>> torch.pow(base, exp)\\n    tensor([  2.,   4.,   8.,  16.])\\n', parsed_docs=[{'header': 'When :attr:`exponent` is a scalar value, the operation applied is', 'content': '\\nath::\\n\\\\text{out}_i = x_i ^ \\\\text{exponent}\\n'}, {'header': 'When :attr:`exponent` is a tensor, the operation applied is', 'content': '\\nath::\\n\\\\text{out}_i = x_i ^ {\\\\text{exponent}_i}\\n\\n :attr:`exponent` is a tensor, the shapes of :attr:`input`\\n:attr:`exponent` must be :ref:`broadcastable <broadcasting-semantics>`.\\n'}, {'header': 'Args', 'content': 'input (Tensor): the input tensor.\\nexponent (float or tensor): the exponent value\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.4331,  1.2475,  0.6834, -0.2791])\\n>>> torch.pow(a, 2)\\ntensor([ 0.1875,  1.5561,  0.4670,  0.0779])\\n>>> exp = torch.arange(1., 5.)\\n\\n>>> a = torch.arange(1., 5.)\\n>>> a\\ntensor([ 1.,  2.,  3.,  4.])\\n>>> exp\\ntensor([ 1.,  2.,  3.,  4.])\\n>>> torch.pow(a, exp)\\ntensor([   1.,    4.,   27.,  256.])\\n\\nunction:: pow(self, exponent, *, out=None) -> Tensor\\nnoindex:\\n\\nr:`self` is a scalar ``float`` value, and :attr:`exponent` is a tensor.\\nreturned tensor :attr:`out` is of the same shape as :attr:`exponent`\\n'}, {'header': 'The operation applied is', 'content': '\\nath::\\n\\\\text{out}_i = \\\\text{self} ^ {\\\\text{exponent}_i}\\n'}, {'header': 'Args', 'content': 'self (float): the scalar base value for the power operation\\nexponent (Tensor): the exponent tensor\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> exp = torch.arange(1., 5.)\\n>>> base = 2\\n>>> torch.pow(base, exp)\\ntensor([  2.,   4.,   8.,  16.])\\n'}], meta_data={}),\n",
       " Item(name='torch.exp', docs='\\nexp(input, *, out=None) -> Tensor\\n\\nReturns a new tensor with the exponential of the elements\\nof the input tensor :attr:`input`.\\n\\n.. math::\\n    y_{i} = e^{x_{i}}\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> torch.exp(torch.tensor([0, math.log(2.)]))\\n    tensor([ 1.,  2.])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> torch.exp(torch.tensor([0, math.log(2.)]))\\ntensor([ 1.,  2.])\\n'}], meta_data={}),\n",
       " Item(name='torch.log', docs='\\nlog(input, *, out=None) -> Tensor\\n\\nReturns a new tensor with the natural logarithm of the elements\\nof :attr:`input`.\\n\\n.. math::\\n    y_{i} = \\\\log_{e} (x_{i})\\n\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.rand(5) * 5\\n    >>> a\\n    tensor([4.7767, 4.3234, 1.2156, 0.2411, 4.5739])\\n    >>> torch.log(a)\\n    tensor([ 1.5637,  1.4640,  0.1952, -1.4226,  1.5204])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.rand(5) * 5\\n>>> a\\ntensor([4.7767, 4.3234, 1.2156, 0.2411, 4.5739])\\n>>> torch.log(a)\\ntensor([ 1.5637,  1.4640,  0.1952, -1.4226,  1.5204])\\n'}], meta_data={}),\n",
       " Item(name='torch.log2', docs='\\nlog2(input, *, out=None) -> Tensor\\n\\nReturns a new tensor with the logarithm to the base 2 of the elements\\nof :attr:`input`.\\n\\n.. math::\\n    y_{i} = \\\\log_{2} (x_{i})\\n\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.rand(5)\\n    >>> a\\n    tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])\\n\\n\\n    >>> torch.log2(a)\\n    tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])\\n\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.rand(5)\\n>>> a\\ntensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])\\n\\n\\n>>> torch.log2(a)\\ntensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])\\n\\n'}], meta_data={}),\n",
       " Item(name='torch.log10', docs='\\nlog10(input, *, out=None) -> Tensor\\n\\nReturns a new tensor with the logarithm to the base 10 of the elements\\nof :attr:`input`.\\n\\n.. math::\\n    y_{i} = \\\\log_{10} (x_{i})\\n\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.rand(5)\\n    >>> a\\n    tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])\\n\\n\\n    >>> torch.log10(a)\\n    tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])\\n\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.rand(5)\\n>>> a\\ntensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])\\n\\n\\n>>> torch.log10(a)\\ntensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])\\n\\n'}], meta_data={}),\n",
       " Item(name='torch.log1p', docs='\\nlog1p(input, *, out=None) -> Tensor\\n\\nReturns a new tensor with the natural logarithm of (1 + :attr:`input`).\\n\\n.. math::\\n    y_i = \\\\log_{e} (x_i + 1)\\n\\n.. note:: This function is more accurate than :func:`torch.log` for small\\n          values of :attr:`input`\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.randn(5)\\n    >>> a\\n    tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])\\n    >>> torch.log1p(a)\\n    tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.randn(5)\\n>>> a\\ntensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])\\n>>> torch.log1p(a)\\ntensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])\\n'}], meta_data={}),\n",
       " Item(name='torch.abs', docs='\\nabs(input, *, out=None) -> Tensor\\n\\nComputes the absolute value of each element in :attr:`input`.\\n\\n.. math::\\n    \\\\text{out}_{i} = |\\\\text{input}_{i}|\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> torch.abs(torch.tensor([-1, -2, 3]))\\n    tensor([ 1,  2,  3])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> torch.abs(torch.tensor([-1, -2, 3]))\\ntensor([ 1,  2,  3])\\n'}], meta_data={}),\n",
       " Item(name='torch.ceil', docs='\\nceil(input, *, out=None) -> Tensor\\n\\nReturns a new tensor with the ceil of the elements of :attr:`input`,\\nthe smallest integer greater than or equal to each element.\\n\\nFor integer inputs, follows the array-api convention of returning a\\ncopy of the input tensor.\\n\\n.. math::\\n    \\\\text{out}_{i} = \\\\left\\\\lceil \\\\text{input}_{i} \\\\right\\\\rceil\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.randn(4)\\n    >>> a\\n    tensor([-0.6341, -1.4208, -1.0900,  0.5826])\\n    >>> torch.ceil(a)\\n    tensor([-0., -1., -1.,  1.])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.randn(4)\\n>>> a\\ntensor([-0.6341, -1.4208, -1.0900,  0.5826])\\n>>> torch.ceil(a)\\ntensor([-0., -1., -1.,  1.])\\n'}], meta_data={}),\n",
       " Item(name='torch.floor', docs='\\nfloor(input, *, out=None) -> Tensor\\n\\nReturns a new tensor with the floor of the elements of :attr:`input`,\\nthe largest integer less than or equal to each element.\\n\\nFor integer inputs, follows the array-api convention of returning a\\ncopy of the input tensor.\\n\\n.. math::\\n    \\\\text{out}_{i} = \\\\left\\\\lfloor \\\\text{input}_{i} \\\\right\\\\rfloor\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.randn(4)\\n    >>> a\\n    tensor([-0.8166,  1.5308, -0.2530, -0.2091])\\n    >>> torch.floor(a)\\n    tensor([-1.,  1., -1., -1.])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.randn(4)\\n>>> a\\ntensor([-0.8166,  1.5308, -0.2530, -0.2091])\\n>>> torch.floor(a)\\ntensor([-1.,  1., -1., -1.])\\n'}], meta_data={}),\n",
       " Item(name='torch.round', docs='\\nround(input, *, decimals=0, out=None) -> Tensor\\n\\nRounds elements of :attr:`input` to the nearest integer.\\n\\nFor integer inputs, follows the array-api convention of returning a\\ncopy of the input tensor.\\nThe return type of output is same as that of input\\'s dtype.\\n\\n.. note::\\n    This function implements the \"round half to even\" to\\n    break ties when a number is equidistant from two\\n    integers (e.g. `round(2.5)` is 2).\\n\\n    When the :attr:\\\\`decimals\\\\` argument is specified the\\n    algorithm used is similar to NumPy\\'s `around`. This\\n    algorithm is fast but inexact and it can easily\\n    overflow for low precision dtypes.\\n    Eg. `round(tensor([10000], dtype=torch.float16), decimals=3)` is `inf`.\\n\\n.. seealso::\\n    :func:`torch.ceil`, which rounds up.\\n    :func:`torch.floor`, which rounds down.\\n    :func:`torch.trunc`, which rounds towards zero.\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n    decimals (int): Number of decimal places to round to (default: 0).\\n        If decimals is negative, it specifies the number of positions\\n        to the left of the decimal point.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> torch.round(torch.tensor((4.7, -2.3, 9.1, -7.7)))\\n    tensor([ 5.,  -2.,  9., -8.])\\n\\n    >>> # Values equidistant from two integers are rounded towards the\\n    >>> #   the nearest even value (zero is treated as even)\\n    >>> torch.round(torch.tensor([-0.5, 0.5, 1.5, 2.5]))\\n    tensor([-0., 0., 2., 2.])\\n\\n    >>> # A positive decimals argument rounds to the to that decimal place\\n    >>> torch.round(torch.tensor([0.1234567]), decimals=3)\\n    tensor([0.1230])\\n\\n    >>> # A negative decimals argument rounds to the left of the decimal\\n    >>> torch.round(torch.tensor([1200.1234567]), decimals=-3)\\n    tensor([1000.])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\ndecimals (int): Number of decimal places to round to (default: 0).\\n    If decimals is negative, it specifies the number of positions\\n    to the left of the decimal point.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> torch.round(torch.tensor((4.7, -2.3, 9.1, -7.7)))\\ntensor([ 5.,  -2.,  9., -8.])\\n\\n>>> # Values equidistant from two integers are rounded towards the\\n>>> #   the nearest even value (zero is treated as even)\\n>>> torch.round(torch.tensor([-0.5, 0.5, 1.5, 2.5]))\\ntensor([-0., 0., 2., 2.])\\n\\n>>> # A positive decimals argument rounds to the to that decimal place\\n>>> torch.round(torch.tensor([0.1234567]), decimals=3)\\ntensor([0.1230])\\n\\n>>> # A negative decimals argument rounds to the left of the decimal\\n>>> torch.round(torch.tensor([1200.1234567]), decimals=-3)\\ntensor([1000.])\\n'}], meta_data={}),\n",
       " Item(name='torch.Tensor.round', docs='\\nround(decimals=0) -> Tensor\\n\\nSee :func:`torch.round`\\n', parsed_docs=[], meta_data={}),\n",
       " Item(name='torch.clamp', docs='\\nclamp(input, min=None, max=None, *, out=None) -> Tensor\\n\\nClamps all elements in :attr:`input` into the range `[` :attr:`min`, :attr:`max` `]`.\\nLetting min_value and max_value be :attr:`min` and :attr:`max`, respectively, this returns:\\n\\n.. math::\\n    y_i = \\\\min(\\\\max(x_i, \\\\text{min\\\\_value}_i), \\\\text{max\\\\_value}_i)\\n\\nIf :attr:`min` is ``None``, there is no lower bound.\\nOr, if :attr:`max` is ``None`` there is no upper bound.\\n\\n\\n.. note::\\n    If :attr:`min` is greater than :attr:`max` :func:`torch.clamp(..., min, max) <torch.clamp>`\\n    sets all elements in :attr:`input` to the value of :attr:`max`.\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n    min (Number or Tensor, optional): lower-bound of the range to be clamped to\\n    max (Number or Tensor, optional): upper-bound of the range to be clamped to\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.randn(4)\\n    >>> a\\n    tensor([-1.7120,  0.1734, -0.0478, -0.0922])\\n    >>> torch.clamp(a, min=-0.5, max=0.5)\\n    tensor([-0.5000,  0.1734, -0.0478, -0.0922])\\n\\n    >>> min = torch.linspace(-1, 1, steps=4)\\n    >>> torch.clamp(a, min=min)\\n    tensor([-1.0000,  0.1734,  0.3333,  1.0000])\\n\\n', parsed_docs=[{'header': 'Letting min_value and max_value be :attr:`min` and :attr:`max`, respectively, this returns', 'content': '\\nath::\\ny_i = \\\\min(\\\\max(x_i, \\\\text{min\\\\_value}_i), \\\\text{max\\\\_value}_i)\\n\\nattr:`min` is ``None``, there is no lower bound.\\nif :attr:`max` is ``None`` there is no upper bound.\\n\\n\\note::\\nIf :attr:`min` is greater than :attr:`max` :func:`torch.clamp(..., min, max) <torch.clamp>`\\nsets all elements in :attr:`input` to the value of :attr:`max`.\\n'}, {'header': 'Args', 'content': 'input (Tensor): the input tensor.\\nmin (Number or Tensor, optional): lower-bound of the range to be clamped to\\nmax (Number or Tensor, optional): upper-bound of the range to be clamped to\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.randn(4)\\n>>> a\\ntensor([-1.7120,  0.1734, -0.0478, -0.0922])\\n>>> torch.clamp(a, min=-0.5, max=0.5)\\ntensor([-0.5000,  0.1734, -0.0478, -0.0922])\\n\\n>>> min = torch.linspace(-1, 1, steps=4)\\n>>> torch.clamp(a, min=min)\\ntensor([-1.0000,  0.1734,  0.3333,  1.0000])\\n\\n'}], meta_data={}),\n",
       " Item(name='torch.minimum', docs='\\nminimum(input, other, *, out=None) -> Tensor\\n\\nComputes the element-wise minimum of :attr:`input` and :attr:`other`.\\n\\n.. note::\\n    If one of the elements being compared is a NaN, then that element is returned.\\n    :func:`minimum` is not supported for tensors with complex dtypes.\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n    other (Tensor): the second input tensor\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.tensor((1, 2, -1))\\n    >>> b = torch.tensor((3, 0, 4))\\n    >>> torch.minimum(a, b)\\n    tensor([1, 0, -1])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\nother (Tensor): the second input tensor\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.tensor((1, 2, -1))\\n>>> b = torch.tensor((3, 0, 4))\\n>>> torch.minimum(a, b)\\ntensor([1, 0, -1])\\n'}], meta_data={}),\n",
       " Item(name='torch.maximum', docs='\\nmaximum(input, other, *, out=None) -> Tensor\\n\\nComputes the element-wise maximum of :attr:`input` and :attr:`other`.\\n\\n.. note::\\n    If one of the elements being compared is a NaN, then that element is returned.\\n    :func:`maximum` is not supported for tensors with complex dtypes.\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n    other (Tensor): the second input tensor\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.tensor((1, 2, -1))\\n    >>> b = torch.tensor((3, 0, 4))\\n    >>> torch.maximum(a, b)\\n    tensor([3, 2, 4])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\nother (Tensor): the second input tensor\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.tensor((1, 2, -1))\\n>>> b = torch.tensor((3, 0, 4))\\n>>> torch.maximum(a, b)\\ntensor([3, 2, 4])\\n'}], meta_data={}),\n",
       " Item(name='torch.cumsum', docs='\\ncumsum(input, dim, *, dtype=None, out=None) -> Tensor\\n\\nReturns the cumulative sum of elements of :attr:`input` in the dimension\\n:attr:`dim`.\\n\\nFor example, if :attr:`input` is a vector of size N, the result will also be\\na vector of size N, with elements.\\n\\n.. math::\\n    y_i = x_1 + x_2 + x_3 + \\\\dots + x_i\\n\\nArgs:\\n    input (Tensor): the input tensor.\\n    dim  (int): the dimension to do the operation over\\n\\nKeyword args:\\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\\n        If specified, the input tensor is casted to :attr:`dtype` before the operation\\n        is performed. This is useful for preventing data type overflows. Default: None.\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> a = torch.randint(1, 20, (10,))\\n    >>> a\\n    tensor([13,  7,  3, 10, 13,  3, 15, 10,  9, 10])\\n    >>> torch.cumsum(a, dim=0)\\n    tensor([13, 20, 23, 33, 46, 49, 64, 74, 83, 93])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\ndim  (int): the dimension to do the operation over\\n'}, {'header': 'Keyword args', 'content': 'dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\\n    If specified, the input tensor is casted to :attr:`dtype` before the operation\\n    is performed. This is useful for preventing data type overflows. Default: None.\\nout (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> a = torch.randint(1, 20, (10,))\\n>>> a\\ntensor([13,  7,  3, 10, 13,  3, 15, 10,  9, 10])\\n>>> torch.cumsum(a, dim=0)\\ntensor([13, 20, 23, 33, 46, 49, 64, 74, 83, 93])\\n'}], meta_data={}),\n",
       " Item(name='torch.arange', docs='\\narange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\\n\\nReturns a 1-D tensor of size :math:`\\\\left\\\\lceil \\\\frac{\\\\text{end} - \\\\text{start}}{\\\\text{step}} \\\\right\\\\rceil`\\nwith values from the interval ``[start, end)`` taken with common difference\\n:attr:`step` beginning from `start`.\\n\\nNote that non-integer :attr:`step` is subject to floating point rounding errors when\\ncomparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end`\\nin such cases.\\n\\n.. math::\\n    \\\\text{out}_{{i+1}} = \\\\text{out}_{i} + \\\\text{step}\\n\\nArgs:\\n    start (Number): the starting value for the set of points. Default: ``0``.\\n    end (Number): the ending value for the set of points\\n    step (Number): the gap between each pair of adjacent points. Default: ``1``.\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\\n        Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input\\n        arguments. If any of `start`, `end`, or `stop` are floating-point, the\\n        `dtype` is inferred to be the default dtype, see\\n        :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to\\n        be `torch.int64`.\\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\\n        Default: ``torch.strided``.\\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\\n        Default: if ``None``, uses the current device for the default tensor type\\n        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\\n    requires_grad (bool, optional): If autograd should record operations on the\\n        returned tensor. Default: ``False``.\\n\\nExample::\\n\\n    >>> torch.arange(5)\\n    tensor([ 0,  1,  2,  3,  4])\\n    >>> torch.arange(1, 4)\\n    tensor([ 1,  2,  3])\\n    >>> torch.arange(1, 2.5, 0.5)\\n    tensor([ 1.0000,  1.5000,  2.0000])\\n', parsed_docs=[{'header': 'Args', 'content': 'start (Number): the starting value for the set of points. Default: ``0``.\\nend (Number): the ending value for the set of points\\nstep (Number): the gap between each pair of adjacent points. Default: ``1``.\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\ndtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\\n    Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input\\n    arguments. If any of `start`, `end`, or `stop` are floating-point, the\\n    `dtype` is inferred to be the default dtype, see\\n    :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to\\n    be `torch.int64`.\\nlayout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\\n    Default: ``torch.strided``.\\ndevice (:class:`torch.device`, optional): the desired device of returned tensor.\\n    Default: if ``None``, uses the current device for the default tensor type\\n    (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\\n    for CPU tensor types and the current CUDA device for CUDA tensor types.\\nrequires_grad (bool, optional): If autograd should record operations on the\\n    returned tensor. Default: ``False``.\\n'}, {'header': 'Example', 'content': '\\n>>> torch.arange(5)\\ntensor([ 0,  1,  2,  3,  4])\\n>>> torch.arange(1, 4)\\ntensor([ 1,  2,  3])\\n>>> torch.arange(1, 2.5, 0.5)\\ntensor([ 1.0000,  1.5000,  2.0000])\\n'}], meta_data={}),\n",
       " Item(name='torch.lerp', docs='\\nlerp(input, end, weight, *, out=None)\\n\\nDoes a linear interpolation of two tensors :attr:`start` (given by :attr:`input`) and :attr:`end` based\\non a scalar or tensor :attr:`weight` and returns the resulting :attr:`out` tensor.\\n\\n.. math::\\n    \\\\text{out}_i = \\\\text{start}_i + \\\\text{weight}_i \\\\times (\\\\text{end}_i - \\\\text{start}_i)\\n\\nThe shapes of :attr:`start` and :attr:`end` must be\\n:ref:`broadcastable <broadcasting-semantics>`. If :attr:`weight` is a tensor, then\\nthe shapes of :attr:`weight`, :attr:`start`, and :attr:`end` must be :ref:`broadcastable <broadcasting-semantics>`.\\n\\nArgs:\\n    input (Tensor): the tensor with the starting points\\n    end (Tensor): the tensor with the ending points\\n    weight (float or tensor): the weight for the interpolation formula\\n\\nKeyword args:\\n    out (Tensor, optional): the output tensor.\\n\\nExample::\\n\\n    >>> start = torch.arange(1., 5.)\\n    >>> end = torch.empty(4).fill_(10)\\n    >>> start\\n    tensor([ 1.,  2.,  3.,  4.])\\n    >>> end\\n    tensor([ 10.,  10.,  10.,  10.])\\n    >>> torch.lerp(start, end, 0.5)\\n    tensor([ 5.5000,  6.0000,  6.5000,  7.0000])\\n    >>> torch.lerp(start, end, torch.full_like(start, 0.5))\\n    tensor([ 5.5000,  6.0000,  6.5000,  7.0000])\\n', parsed_docs=[{'header': 'Args', 'content': 'input (Tensor): the tensor with the starting points\\nend (Tensor): the tensor with the ending points\\nweight (float or tensor): the weight for the interpolation formula\\n'}, {'header': 'Keyword args', 'content': 'out (Tensor, optional): the output tensor.\\n'}, {'header': 'Example', 'content': '\\n>>> start = torch.arange(1., 5.)\\n>>> end = torch.empty(4).fill_(10)\\n>>> start\\ntensor([ 1.,  2.,  3.,  4.])\\n>>> end\\ntensor([ 10.,  10.,  10.,  10.])\\n>>> torch.lerp(start, end, 0.5)\\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\\n>>> torch.lerp(start, end, torch.full_like(start, 0.5))\\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\\n'}], meta_data={}),\n",
       " Item(name='F.one_hot', docs='\\none_hot(tensor, num_classes=-1) -> LongTensor\\n\\nTakes LongTensor with index values of shape ``(*)`` and returns a tensor\\nof shape ``(*, num_classes)`` that have zeros everywhere except where the\\nindex of last dimension matches the corresponding value of the input tensor,\\nin which case it will be 1.\\n\\nSee also `One-hot on Wikipedia`_ .\\n\\n.. _One-hot on Wikipedia:\\n    https://en.wikipedia.org/wiki/One-hot\\n\\nArguments:\\n    tensor (LongTensor): class values of any shape.\\n    num_classes (int):  Total number of classes. If set to -1, the number\\n        of classes will be inferred as one greater than the largest class\\n        value in the input tensor.\\n\\nReturns:\\n    LongTensor that has one more dimension with 1 values at the\\n    index of last dimension indicated by the input, and 0 everywhere\\n    else.\\n\\nExamples:\\n    >>> F.one_hot(torch.arange(0, 5) % 3)\\n    tensor([[1, 0, 0],\\n            [0, 1, 0],\\n            [0, 0, 1],\\n            [1, 0, 0],\\n            [0, 1, 0]])\\n    >>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\\n    tensor([[1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0],\\n            [0, 0, 1, 0, 0],\\n            [1, 0, 0, 0, 0],\\n            [0, 1, 0, 0, 0]])\\n    >>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\\n    tensor([[[1, 0, 0],\\n             [0, 1, 0]],\\n            [[0, 0, 1],\\n             [1, 0, 0]],\\n            [[0, 1, 0],\\n             [0, 0, 1]]])\\n', parsed_docs=[{'header': 'Arguments', 'content': 'tensor (LongTensor): class values of any shape.\\nnum_classes (int):  Total number of classes. If set to -1, the number\\n    of classes will be inferred as one greater than the largest class\\n    value in the input tensor.\\n'}, {'header': 'Returns', 'content': 'LongTensor that has one more dimension with 1 values at the\\nindex of last dimension indicated by the input, and 0 everywhere\\nelse.\\n'}, {'header': 'Examples', 'content': '>>> F.one_hot(torch.arange(0, 5) % 3)\\ntensor([[1, 0, 0],\\n        [0, 1, 0],\\n        [0, 0, 1],\\n        [1, 0, 0],\\n        [0, 1, 0]])\\n>>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\\ntensor([[1, 0, 0, 0, 0],\\n        [0, 1, 0, 0, 0],\\n        [0, 0, 1, 0, 0],\\n        [1, 0, 0, 0, 0],\\n        [0, 1, 0, 0, 0]])\\n>>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\\ntensor([[[1, 0, 0],\\n         [0, 1, 0]],\\n        [[0, 0, 1],\\n         [1, 0, 0]],\\n        [[0, 1, 0],\\n         [0, 0, 1]]])\\n'}], meta_data={}),\n",
       " Item(name='torch.cdist', docs=\"Computes batched the p-norm distance between each pair of the two collections of row vectors.\\n\\n    Args:\\n        x1 (Tensor): input tensor of shape :math:`B \\\\times P \\\\times M`.\\n        x2 (Tensor): input tensor of shape :math:`B \\\\times R \\\\times M`.\\n        p: p value for the p-norm distance to calculate between each vector pair\\n            :math:`\\\\in [0, \\\\infty]`.\\n        compute_mode:\\n            'use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate\\n            euclidean distance (p = 2) if P > 25 or R > 25\\n            'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate\\n            euclidean distance (p = 2)\\n            'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate\\n            euclidean distance (p = 2)\\n            Default: use_mm_for_euclid_dist_if_necessary.\\n\\n    If x1 has shape :math:`B \\\\times P \\\\times M` and x2 has shape :math:`B \\\\times R \\\\times M` then the\\n    output will have shape :math:`B \\\\times P \\\\times R`.\\n\\n    This function is equivalent to `scipy.spatial.distance.cdist(input,'minkowski', p=p)`\\n    if :math:`p \\\\in (0, \\\\infty)`. When :math:`p = 0` it is equivalent to\\n    `scipy.spatial.distance.cdist(input, 'hamming') * M`. When :math:`p = \\\\infty`, the closest\\n    scipy function is `scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max())`.\\n\\n    Example:\\n\\n        >>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\\n        >>> a\\n        tensor([[ 0.9041,  0.0196],\\n                [-0.3108, -2.4423],\\n                [-0.4821,  1.0590]])\\n        >>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\\n        >>> b\\n        tensor([[-2.1763, -0.4713],\\n                [-0.6986,  1.3702]])\\n        >>> torch.cdist(a, b, p=2)\\n        tensor([[3.1193, 2.0959],\\n                [2.7138, 3.8322],\\n                [2.2830, 0.3791]])\\n    \", parsed_docs=[{'header': 'Args', 'content': \"x1 (Tensor): input tensor of shape :math:`B \\\\times P \\\\times M`.\\nx2 (Tensor): input tensor of shape :math:`B \\\\times R \\\\times M`.\\np: p value for the p-norm distance to calculate between each vector pair\\n    :math:`\\\\in [0, \\\\infty]`.\\ncompute_mode:\\n    'use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate\\n    euclidean distance (p = 2) if P > 25 or R > 25\\n    'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate\\n    euclidean distance (p = 2)\\n    'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate\\n    euclidean distance (p = 2)\\n    Default: use_mm_for_euclid_dist_if_necessary.\\n\\n1 has shape :math:`B \\\\times P \\\\times M` and x2 has shape :math:`B \\\\times R \\\\times M` then the\\nut will have shape :math:`B \\\\times P \\\\times R`.\\n\\n function is equivalent to `scipy.spatial.distance.cdist(input,'minkowski', p=p)`\\nmath:`p \\\\in (0, \\\\infty)`. When :math:`p = 0` it is equivalent to\\npy.spatial.distance.cdist(input, 'hamming') * M`. When :math:`p = \\\\infty`, the closest\\ny function is `scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max())`.\\n\"}, {'header': 'Example', 'content': '\\n>>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\\n>>> a\\ntensor([[ 0.9041,  0.0196],\\n        [-0.3108, -2.4423],\\n        [-0.4821,  1.0590]])\\n>>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\\n>>> b\\ntensor([[-2.1763, -0.4713],\\n        [-0.6986,  1.3702]])\\n>>> torch.cdist(a, b, p=2)\\ntensor([[3.1193, 2.0959],\\n        [2.7138, 3.8322],\\n        [2.2830, 0.3791]])\\n'}], meta_data={})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemholder.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"mongodb+srv://sisungkim:skrkwh8327@cluster0.mkj2sh1.mongodb.net/\"\n",
    "import pymongo\n",
    "\n",
    "myclient = pymongo.MongoClient(connection_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"nobuco_pm\"\n",
    "col_name = \"db_manager\"\n",
    "\n",
    "mydb = myclient[db_name]\n",
    "mycol = mydb[col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in mycol.find():\n",
    "    retrieved.append(x)\n",
    "    \n",
    "retrieved = retrieved[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_inputs_blocks(generated_text):\n",
    "    lines = generated_text.split('\\n')\n",
    "    inputs_blocks = []\n",
    "    current_block = []\n",
    "    inside_block = False\n",
    "    open_bracket_count = 0\n",
    "\n",
    "    for line in lines:\n",
    "        if 'inputs = [' in line:\n",
    "            inside_block = True\n",
    "            current_block.append(line)\n",
    "            open_bracket_count += line.count('[')\n",
    "            open_bracket_count -= line.count(']')\n",
    "        elif inside_block:\n",
    "            current_block.append(line)\n",
    "            open_bracket_count += line.count('[')\n",
    "            open_bracket_count -= line.count(']')\n",
    "            if open_bracket_count == 0:\n",
    "                inputs_blocks.append('\\n'.join(current_block))\n",
    "                current_block = []\n",
    "                inside_block = False\n",
    "\n",
    "    return inputs_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_input_blocks(generated_text):\n",
    "    input_blocks = re.findall(r'^input\\s*=\\s*.*', generated_text, re.MULTILINE)\n",
    "    return input_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_generated_inputs(generated_text):\n",
    "    generated_inputs = []\n",
    "    input_blocks = extract_input_blocks(generated_text)\n",
    "    inputs_blocks = extract_inputs_blocks(generated_text)\n",
    "    generated_inputs.extend(input_blocks)\n",
    "    generated_inputs.extend(inputs_blocks)\n",
    "    return generated_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_func(func_name):\n",
    "    split = str(func_name).split(\".\")\n",
    "    if \"Tensor\" in split:\n",
    "        func = getattr(torch.Tensor, split[-1])\n",
    "    elif \"F\" in split:\n",
    "        try:\n",
    "            func = getattr(torch.nn.functional, split[-1])\n",
    "        except:\n",
    "            try:\n",
    "                func = getattr(torch.functional, split[-1])\n",
    "            except:\n",
    "                func = None\n",
    "    elif \"nn\" in split:\n",
    "        func = getattr(torch.nn, split[-1])\n",
    "    else:\n",
    "        func = getattr(torch, split[-1])\n",
    "\n",
    "    return func\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('6655eed7698f3b981e49670b'),\n",
       " 'project_name': 'unittest_automation_test_initiation',\n",
       " 'project_id': '0',\n",
       " 'task': 'function_input_generation',\n",
       " 'name': 'torch.cos',\n",
       " 'example': '\\nGenerate simple inputs to run the target code block.\\n\\ntarget:\\n```\\ntorch.Tensor.add(*inputs)\\n```\\n\\noutput:\\n```python\\ninputs = [\\n    torch.randn(1, 20, 32),\\n    torch.randn(1, 20, 32)\\n]\\n```\\n\\n',\n",
       " 'docs': [{'header': 'Args', 'content': 'input (Tensor): the input tensor.\\n'},\n",
       "  {'header': 'Keyword args',\n",
       "   'content': 'out (Tensor, optional): the output tensor.\\n'},\n",
       "  {'header': 'Example',\n",
       "   'content': '\\n>>> a = torch.randn(4)\\n>>> a\\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\\n>>> torch.cos(a)\\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])\\n'}],\n",
       " 'generated_text': 'Here is a simple input to run the target code block.\\n\\ninputs = [\\n    torch.tensor([0.5, 1.2, -3.7, 0.])\\n]',\n",
       " 'generated_inputs': ['inputs = [\\n    torch.tensor([0.5, 1.2, -3.7, 0.]']}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in retrieved:\n",
    "    data[\"generated_inputs\"] = extract_generated_inputs(data[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_refs(generated_input):\n",
    "    if \"inputs\" in generated_input:\n",
    "        return [\"inputs\", \"*inputs\", \"inputs\"]\n",
    "    else:\n",
    "        return [\"input\"] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indent(text: str, n: int=4) -> str:\n",
    "    # n 만큼의 공백 생성\n",
    "    indent = ' ' * n\n",
    "    # 각 줄의 시작 부분에 공백 추가\n",
    "    indented_text = '\\n'.join(indent + line for line in text.split('\\n'))\n",
    "    return indented_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autopep8\n",
    "\n",
    "def reindent_code(code: str) -> str:\n",
    "    # Use autopep8 to format the code according to PEP 8\n",
    "    formatted_code = autopep8.fix_code(code)\n",
    "    return formatted_code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_target_for_func_name(target):\n",
    "    return target.replace('.', '_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unittest_template(target, input_declaration):\n",
    "    target_in_func = convert_target_for_func_name(target)\n",
    "    input_refs = generate_input_refs(input_declaration)\n",
    "    \n",
    "    unittest =  f\"\"\"\n",
    "def test_{target_in_func}_converter(self):\n",
    "    # Initialize the model directly from its constructor\n",
    "    \n",
    "    class MyModule(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "        \n",
    "        def forward(self, {input_refs[0]}):\n",
    "            {target}({input_refs[1]})\n",
    "    \n",
    "    torch_model = MyModule()\n",
    "    \n",
    "    torch_model.eval()\n",
    "\t# Initialize the model and input tensor\n",
    "    {input_declaration}\n",
    "\n",
    "    # Convert the model and ensure the HTML trace is saved\n",
    "    keras_model = nobuco.pytorch_to_keras(\n",
    "        torch_model,\n",
    "        args=[{input_refs[2]}], kwargs=None,\n",
    "        inputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
    "        outputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
    "        save_trace_html=True\n",
    "    )\n",
    "\n",
    "    # Read the contents of the trace.html file\n",
    "    with open('trace.html', 'r', encoding='utf-8') as file:\n",
    "        trace_html = file.read()\n",
    "\n",
    "    # Assertions for the content of trace_html\n",
    "    self.assertNotIn('Max diff', trace_html, \"The trace HTML should not contain 'Max diff'\")\n",
    "\n",
    "\"\"\"\n",
    "    return reindent_code(unittest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch_sin'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved[0][\"name\"].replace('.', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def test_torch_add_converter(self):\n",
      "    # Initialize the model directly from its constructor\n",
      "\n",
      "    class MyModule(torch.nn.Module):\n",
      "        def __init__(self):\n",
      "            super().__init__()\n",
      "\n",
      "        def forward(self, inputs):\n",
      "            torch.add(*inputs)\n",
      "\n",
      "    torch_model = MyModule()\n",
      "\n",
      "    torch_model.eval()\n",
      "    # Initialize the model and input tensor\n",
      "    inputs = [\n",
      "        torch.randn(4),\n",
      "        10\n",
      "    ]\n",
      "\n",
      "    # Convert the model and ensure the HTML trace is saved\n",
      "    keras_model = nobuco.pytorch_to_keras(\n",
      "        torch_model,\n",
      "        args=[inputs], kwargs=None,\n",
      "        inputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
      "        outputs_channel_order=nobuco.ChannelOrder.TENSORFLOW,\n",
      "        save_trace_html=True\n",
      "    )\n",
      "\n",
      "    # Read the contents of the trace.html file\n",
      "    with open('trace.html', 'r', encoding='utf-8') as file:\n",
      "        trace_html = file.read()\n",
      "\n",
      "    # Assertions for the content of trace_html\n",
      "    self.assertNotIn('Max diff', trace_html,\n",
      "                     \"The trace HTML should not contain 'Max diff'\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 2\n",
    "\n",
    "print(generate_unittest_template(retrieved[index][\"name\"], retrieved[index][\"generated_inputs\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unittest for torch.sin was added\n",
      "unittest for torch.cos was added\n",
      "unittest for torch.add was added\n",
      "unittest for torch.sub was added\n",
      "unittest for torch.Tensor.neg was failed list index out of range\n",
      "unittest for torch.mul was added\n",
      "unittest for torch.Tensor.mul was added\n",
      "unittest for torch.Tensor.div was added\n",
      "unittest for torch.floor_divide was added\n",
      "unittest for torch.sqrt was added\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unittest for torch.rsqrt was added\n",
      "unittest for torch.square was added\n",
      "unittest for torch.pow was added\n",
      "unittest for torch.exp was added\n",
      "unittest for torch.log was failed list index out of range\n",
      "unittest for torch.log2 was failed list index out of range\n",
      "unittest for torch.log10 was added\n",
      "unittest for torch.log1p was added\n",
      "unittest for torch.abs was added\n",
      "unittest for torch.ceil was added\n",
      "unittest for torch.floor was added\n",
      "unittest for torch.round was added\n",
      "unittest for torch.Tensor.round was added\n",
      "unittest for torch.clamp was added\n",
      "unittest for torch.minimum was added\n",
      "unittest for torch.maximum was added\n",
      "unittest for torch.cumsum was added\n",
      "unittest for torch.arange was added\n",
      "unittest for torch.lerp was added\n",
      "unittest for F.one_hot was added\n",
      "unittest for torch.cdist was added\n",
      "unittest for torch.sign was added\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unittest_start = '''\n",
    "import unittest\n",
    "import torch\n",
    "import nobuco\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TestAutogenatedUnittests(unittest.TestCase):\n",
    "'''\n",
    "\n",
    "unittest_end = '''\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "\n",
    "'''\n",
    "\n",
    "for data in retrieved:\n",
    "    try:\n",
    "        unittest_code = unittest_start + add_indent(\n",
    "            generate_unittest_template(\n",
    "                data[\"name\"],\n",
    "                data[\"generated_inputs\"][0]\n",
    "            )\n",
    "        ) + unittest_end\n",
    "        \n",
    "        target_in_func = convert_target_for_func_name(data[\"name\"])\n",
    "        \n",
    "        file_name = f\"test_{target_in_func}.py\"\n",
    "        dir_path  = \"test\"\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        # Write the generated unittest to a file\n",
    "        with open(os.path.join(dir_path, file_name), 'w') as file:\n",
    "            file.write(unittest_code)\n",
    "    \n",
    "        print(f'unittest for {data[\"name\"]} was added')\n",
    "    except Exception as error:\n",
    "        print(f'unittest for {data[\"name\"]} was failed', error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
